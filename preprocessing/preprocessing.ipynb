{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb861e81-8564-4107-82cf-b14a07c4834a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Setting up Spark Session / Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d52b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, BooleanType, DoubleType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756be08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark cluster init\n",
    "def bytes_to_mb(size_bytes):\n",
    "    \"\"\"Converts bytes to megabytes.\"\"\"\n",
    "    return size_bytes / (1024 * 1024)\n",
    "\n",
    "def bytes_to_gb(size_bytes):\n",
    "    \"\"\"Converts bytes to gigabytes.\"\"\"\n",
    "    return size_bytes / (1024 * 1024 * 1024)\n",
    "\n",
    "def configure_spark(dataset_size_gb):\n",
    "    \"\"\"\n",
    "    Configures Spark executor cores and memory based on the dataset size.\n",
    "\n",
    "    The number of executor cores is calculated using a linear relationship with the dataset size (in GB),\n",
    "    ensuring the result is an even number. Executor memory is determined based on the number of executor cores,\n",
    "    with a maximum memory limit.\n",
    "\n",
    "    Args:\n",
    "        dataset_size_gb (float): Size of the dataset in gigabytes.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - executor_cores (int): The number of executor cores.\n",
    "            - executor_memory (str): The executor memory configuration (e.g., \"4g\").\n",
    "\n",
    "    Configuration Details:\n",
    "        - Executor cores are calculated as: `executor_cores = int(dataset_size_gb * core_factor) + 2`\n",
    "        - `core_factor` is set to 2.\n",
    "        - The calculated `executor_cores` is then adjusted to the next even number if it is odd.\n",
    "        - Executor memory is determined by: `executor_memory = f\"{min(executor_cores, 4) * memory_factor}g\"`\n",
    "        - `memory_factor` is set to 1.\n",
    "        - The memory value is capped, ensuring it doesn't exceed 4GB if `executor_cores` is greater than 4.\n",
    "    \"\"\"\n",
    "\n",
    "    core_factor   = 2\n",
    "    memory_factor = 1\n",
    "    executor_cores  = int(dataset_size_gb * core_factor) + 2\n",
    "    # Ensure executor_cores is even\n",
    "    if executor_cores % 2 != 0:\n",
    "        executor_cores += 1\n",
    "    executor_memory = f\"{min(executor_cores, 4) * memory_factor}g\"\n",
    "    \n",
    "    return executor_cores, executor_memory\n",
    "\n",
    "\n",
    "def build_spark_session(hdfs_path, file_path, verbose=False):\n",
    "    \"\"\"Builds a Spark session and retrieves file size from HDFS.\n",
    "\n",
    "    Args:\n",
    "        hdfs_path (str): HDFS path.\n",
    "        file_path (str): File path within HDFS.\n",
    "        verbose (bool, optional): Enable verbose output. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: SparkSession, SparkContext, and file size.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.appName(\"Project Group 32 HDFSFileSize\").getOrCreate()\n",
    "    jvm = spark._jvm\n",
    "    conf = jvm.org.apache.hadoop.conf.Configuration()\n",
    "    fs = jvm.org.apache.hadoop.fs.FileSystem.get(jvm.java.net.URI.create(hdfs_path), conf)\n",
    "    path = jvm.org.apache.hadoop.fs.Path(file_path)\n",
    "    fileStatus = fs.getFileStatus(path)\n",
    "    fileSize = fileStatus.getLen()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"File size in bytes: {fileSize}\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "    executor_cores, executor_memory = configure_spark(bytes_to_gb(fileSize))\n",
    "\n",
    "    \"\"\"\n",
    "    Configuration Parameters:\n",
    "        - master (str): The master URL for the cluster. In this case, it's \"spark://192.168.2.156:7077\",\n",
    "        indicating a Spark standalone cluster.\n",
    "        - appName (str): The name of the Spark application, set to \"Project Group 32\".\n",
    "        - spark.dynamicAllocation.enabled (bool): Enables or disables dynamic allocation of executors.\n",
    "        Set to True, allowing Spark to adjust the number of executors dynamically based on workload.\n",
    "        - spark.dynamicAllocation.shuffleTracking.enabled (bool): Enables or disables shuffle tracking for dynamic allocation.\n",
    "        Set to True, which is often necessary for dynamic allocation to work correctly with external shuffle services.\n",
    "        - spark.shuffle.service.enabled (bool): Enables or disables an external shuffle service.\n",
    "        Set to False, meaning the shuffle operations will be handled by the executors themselves.\n",
    "        - spark.dynamicAllocation.executorIdleTimeout (str):  Specifies the duration for which an executor can be idle before being removed.\n",
    "        Set to \"60s\", meaning executors idle for 60 seconds will be reclaimed.\n",
    "        - spark.executor.cores (int): The number of cores allocated to each executor.\n",
    "        Set to the minimum of `executor_cores` and 4.  This limits the number of cores per executor to a maximum of 4.\n",
    "        - spark.executor.memory (str): The amount of memory allocated to each executor.\n",
    "        Set to the value of `executor_memory`, which should be a string representing the memory size (e.g., \"4g\").\n",
    "        - spark.cores.max (int): The maximum number of total cores to use for the application.\n",
    "        Set to the minimum of `executor_cores` and 32. This limits the total number of cores the application can request to a maximum of 32.\n",
    "        - spark.driver.port (int): The port used by the Spark driver process.\n",
    "        Set to 9999.\n",
    "        - spark.blockManager.port (int): The port used by the Spark block manager.\n",
    "        Set to 10005.\n",
    "    \"\"\"\n",
    "\n",
    "    spark_session = SparkSession.builder\\\n",
    "            .master(\"spark://192.168.2.156:7077\") \\\n",
    "            .appName(\"Project Group 32\")\\\n",
    "            .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "            .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "            .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "            .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"60s\")\\\n",
    "            .config(\"spark.executor.cores\", min(executor_cores, 4))\\\n",
    "            .config(\"spark.executor.memory\", executor_memory)\\\n",
    "            .config(\"spark.cores.max\", min(executor_cores, 32))\\\n",
    "            .config(\"spark.driver.port\",9999)\\\n",
    "            .config(\"spark.blockManager.port\",10005)\\\n",
    "            .getOrCreate()\n",
    "\n",
    "    # RDD API\n",
    "    spark_context = spark_session.sparkContext\n",
    "    spark_context.setLogLevel(\"ERROR\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"A files size of {bytes_to_gb(fileSize):.4f} GB give a maximum \\n\"+\n",
    "              f\"of {spark_session.conf.get('spark.cores.max')} cores divided on spark executors with:\\n\"+\n",
    "            f\"Executor cores: {spark_session.conf.get('spark.executor.cores')}\\n\"+\n",
    "            f\"Executor memory: {spark_session.conf.get('spark.executor.memory')}\\n\"+\n",
    "            f\"Mem/core: {int(spark_session.conf.get('spark.executor.memory')[:-1])/int(spark_session.conf.get('spark.executor.cores')):.0f}GB\")\n",
    "\n",
    "    return spark_session, spark_context, fileSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db317b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(spark_session, hdfs_path, file_path, schema=None, verbose=False):\n",
    "    \"\"\"Loads JSON data from HDFS into a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spark_session (SparkSession): Spark session.\n",
    "        hdfs_path (str): HDFS path.\n",
    "        file_path (str): File path within HDFS.\n",
    "        schema: Schema for the JSON object. Defaults to None which infers schema from the data.\n",
    "        verbose (bool, optional): Enable verbose output. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Loaded Spark DataFrame.\n",
    "    \"\"\"\n",
    "    # Load JSON file into a Spark DataFrame\n",
    "    if schema is None:\n",
    "        df = spark_session.read.json(hdfs_path + file_path)\n",
    "        if verbose:\n",
    "            # Show schema to understand the structure\n",
    "            print(\"The schema:\")\n",
    "            df.printSchema()\n",
    "            print(\"\\n\")\n",
    "    else:\n",
    "        df = spark_session.read.json(hdfs_path + file_path, schema=schema)\n",
    "\n",
    "    if verbose:\n",
    "        # Count the number of partitions in the underlying RDD.\n",
    "        print(f\"Number of default partitions after loading the data: {df.rdd.getNumPartitions()}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae10602-3066-4837-b87e-dbf81fae1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df):\n",
    "    \"\"\"\n",
    "    Filters a PySpark DataFrame to include only rows from the top 25 most frequent subreddits,\n",
    "    after removing rows with null values in 'subreddit', 'summary', or 'content' columns.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): The input PySpark DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: A filtered PySpark DataFrame containing only rows:\n",
    "            - where 'subreddit', 'summary', and 'content' are not null.\n",
    "            - where 'subreddit' is one of the top 25 most frequent subreddits.\n",
    "\n",
    "    Steps:\n",
    "        1. Filters out rows with null values in 'subreddit', 'summary', or 'content'.\n",
    "        2. Groups the filtered DataFrame by 'subreddit', counts the occurrences, and orders them in descending order.\n",
    "        3. Retrieves the top 25 subreddits based on the counts.\n",
    "        4. Filters the null-filtered DataFrame to include only rows where 'subreddit' is in the top 25 list.\n",
    "    \"\"\"\n",
    "    # Filter out NULL subreddit, summary, or content\n",
    "    df_filtered = df.filter((col(\"subreddit\").isNotNull()) & (col(\"summary\").isNotNull()) & (col(\"content\").isNotNull()))\n",
    "\n",
    "    # Group on subreddit and create a \"count\" for each in descending order\n",
    "    df_counts = df_filtered.groupBy(\"subreddit\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "    # Retrieve the top 25 subreddits\n",
    "    top_25_counts = df_counts.limit(25)\n",
    "    \n",
    "    # Collect (transfer them locally, not distributed) and put into a list\n",
    "    top_25_subreddits = [row.subreddit for row in top_25_counts.collect()]\n",
    "\n",
    "    # Filter the null-filtered data based on the top 25 subreddits\n",
    "    df_filtered = df_filtered.filter(col(\"subreddit\").isin(top_25_subreddits))\n",
    "\n",
    "    return df_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1cab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, seed=42, test_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Splits a PySpark DataFrame into training and test sets using random sampling.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): The input PySpark DataFrame.\n",
    "        seed (int, optional): The seed for the random number generator, ensuring reproducibility. Defaults to 42.\n",
    "        test_fraction (float, optional): The fraction of data to be included in the test set. Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two PySpark DataFrames: (train_data, test_data).\n",
    "            - train_data: The training set DataFrame.\n",
    "            - test_data: The test set DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> train, test = split_data(my_dataframe, seed=123, test_fraction=0.3)\n",
    "        >>> print(f\"Training set count: {train.count()}\")\n",
    "        >>> print(f\"Test set count: {test.count()}\")\n",
    "    \"\"\"\n",
    "    # Split data into training and test sets\n",
    "    train_data, test_data = df.randomSplit([(1-test_fraction), test_fraction], seed=seed)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5f5d6e9-40b1-498d-b9c2-e48ed53478c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_pipe():\n",
    "    \"\"\"\n",
    "    Creates a pipeline for pre-processing text data for machine learning.\n",
    "\n",
    "    This pipeline includes tokenization, stop word removal, TF-IDF vectorization,\n",
    "    label indexing, and feature assembly.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Spark ML pipeline stages.\n",
    "    \"\"\"\n",
    "    # Tokenize summary and content\n",
    "    tokenizer  = Tokenizer(inputCol=\"summary\", outputCol=\"summary_tokens\")\n",
    "    tokenizer2 = Tokenizer(inputCol=\"content\", outputCol=\"content_tokens\")\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords_remover  = StopWordsRemover(inputCol=\"summary_tokens\", outputCol=\"summary_clean\")\n",
    "    stopwords_remover2 = StopWordsRemover(inputCol=\"content_tokens\", outputCol=\"content_clean\")\n",
    "\n",
    "    # Convert words to numerical features using TF-IDF\n",
    "    hashing_tf = HashingTF(inputCol=\"summary_clean\", outputCol=\"summary_tf\", numFeatures=1000)\n",
    "    idf = IDF(inputCol=\"summary_tf\", outputCol=\"summary_features\")\n",
    "\n",
    "    hashing_tf2 = HashingTF(inputCol=\"content_clean\", outputCol=\"content_tf\", numFeatures=1000)\n",
    "    idf2 = IDF(inputCol=\"content_tf\", outputCol=\"content_features\")\n",
    "\n",
    "    # Convert subreddit (text label) into a numerical label\n",
    "    label_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "\n",
    "    # Combine summary and content features\n",
    "    feature_assembler = VectorAssembler(inputCols=[\"summary_features\", \"content_features\"], outputCol=\"features\")\n",
    "\n",
    "    # Return pre-processing pipeline.\n",
    "    return [tokenizer, tokenizer2, stopwords_remover, stopwords_remover2,\n",
    "            hashing_tf, idf, hashing_tf2, idf2, label_indexer, feature_assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, test_data, description=\"\", verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluates a machine learning model's accuracy on test data.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Spark ML model.\n",
    "        test_data (DataFrame): The test dataset.\n",
    "        description (str, optional): A description of the model for output. Defaults to \"\".\n",
    "        verbose (bool, optional): Enable verbose output. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model.\n",
    "    \"\"\"\n",
    "    # Make predictions on test data\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Evaluate model accuracy\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "    if verbose:\n",
    "      print(f\"Evaluation of {description}. \\n\"+\n",
    "            f\"Model Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b65ad2-c53e-438c-878e-e5b2b416302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(train_data, pre_pipe):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest classification model.\n",
    "\n",
    "    Args:\n",
    "        train_data (DataFrame): The training dataset.\n",
    "        pre_pipe (list): List of pre-processing stages.\n",
    "\n",
    "    Returns:\n",
    "        PipelineModel: The trained Random Forest model.\n",
    "    \"\"\"\n",
    "    # Define the Random Forest classifier\n",
    "    classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
    "\n",
    "    # Create a new pipeline using Random Forest\n",
    "    pipeline = Pipeline(stages= pre_pipe + [classifier])\n",
    "\n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    # Save the trained model\n",
    "    #model.save(\"hdfs://192.168.2.156:9000/data/reddit/model/reddit_text_classifier_rf\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d768be0-7041-48a0-9126-10b7857b53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(train_data, pre_pipe):\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression classification model.\n",
    "\n",
    "    Args:\n",
    "        train_data (DataFrame): The training dataset.\n",
    "        pre_pipe (list): List of pre-processing stages.\n",
    "\n",
    "    Returns:\n",
    "        PipelineModel: The trained Logistic Regression model.\n",
    "    \"\"\"\n",
    "    # Define the classification model\n",
    "    classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "    # Create a new pipeline using Logistic Regression\n",
    "    pipeline = Pipeline(stages= pre_pipe + [classifier])\n",
    "\n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    # Save the trained model\n",
    "    # model.save(\"hdfs://192.168.2.156:9000/data/reddit/model/reddit_text_classifier\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebfc4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(i, fileSize, executor_cores, executor_memory, max_cores, data_load_time, training_time, evaluation_time, overall_exec_time, model_accuracy):\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Spark Processing and Model Evaluation Results\\n\")\n",
    "    print(f\"Iteration {i}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"File Size:        {fileSize:.2f} GB\")\n",
    "    print(f\"Max cores:        {max_cores}\")\n",
    "    print(f\"Executor Cores:   {executor_cores}\")\n",
    "    print(f\"Executor Memory:  {executor_memory}\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Performance Metrics:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"Data load Time:         {data_load_time:.2f} seconds\")\n",
    "    print(f\"Training Time:          {training_time:.2f} seconds\")\n",
    "    print(f\"Evaluation Time:        {evaluation_time:.2f} seconds\")\n",
    "    print(f\"Overall execution Time: {overall_exec_time:.2f} seconds\")\n",
    "    print(f\"Model Accuracy:         {model_accuracy:.4f}\")\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "769f1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(hdfs_path, file_path, schema, n=5, verbose=False):\n",
    "\n",
    "    overall_exec_time   = np.zeros(n)\n",
    "    data_load_time      = np.zeros(n)\n",
    "    training_time       = np.zeros(n)\n",
    "    evaluation_time     = np.zeros(n)\n",
    "    model_accuracy      = np.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        print(f\"File {file_path}, run {i}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create a spark session\n",
    "        spark_session, spark_context, fileSize = build_spark_session(hdfs_path, file_path, verbose=verbose)\n",
    "        \n",
    "        # Load the data\n",
    "        df = load_data(spark_session, hdfs_path, file_path, fileSize, schema=schema, verbose=verbose)\n",
    "\n",
    "        # Filter the data\n",
    "        df = filter_data(df)\n",
    "\n",
    "        # Split data into training and test sets\n",
    "        train_data, test_data = split_data(df)\n",
    "\n",
    "        # Save time for data load/transform\n",
    "        data_time = time.time()\n",
    "        data_load_time[i] = data_time - start_time\n",
    "\n",
    "        # Create a pipeline for the pre-processing\n",
    "        pre_pipe = pre_processing_pipe()\n",
    "        # Create and train a ML model for classification\n",
    "        #model = random_forest(train_data, pre_pipe)\n",
    "        model = logistic_regression(train_data, pre_pipe)\n",
    "\n",
    "        # Save time for model training\n",
    "        train_time = time.time()\n",
    "        training_time[i] = train_time - data_time\n",
    "\n",
    "        # Evaluate the performance of the ML model on the test data\n",
    "        #model_accuracy[i] = model_eval(model, test_data, description=\"Random forest classifier\", verbose=verbose)\n",
    "        model_accuracy[i] = model_eval(model, test_data, description=\"Logistic regression classifier\", verbose=verbose)\n",
    "\n",
    "        # Save time for model evaluation\n",
    "        eval_time = time.time()\n",
    "        evaluation_time[i] = eval_time - train_time\n",
    "\n",
    "        executor_cores = spark_session.conf.get(\"spark.executor.cores\")\n",
    "        executor_memory = spark_session.conf.get(\"spark.executor.memory\")\n",
    "        max_cores = spark_session.conf.get('spark.cores.max')\n",
    "\n",
    "        spark_context.stop()\n",
    "\n",
    "        # Determine overall execution time\n",
    "        end_time = time.time()\n",
    "        overall_exec_time[i] = end_time - start_time\n",
    "        \n",
    "        if verbose:\n",
    "            print_results(i, bytes_to_gb(fileSize), executor_cores, executor_memory, max_cores, data_load_time[i], training_time[i], evaluation_time[i], overall_exec_time[i], model_accuracy[i])\n",
    "\n",
    "    return [bytes_to_gb(fileSize), executor_cores, executor_memory, max_cores, data_load_time.mean(), training_time.mean(), evaluation_time.mean(), overall_exec_time.mean(), model_accuracy.mean()]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38f32162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schemas for the reddit data\n",
    "subreddit_field = StructField(name=\"subreddit\", dataType=StringType(), nullable=True)\n",
    "summary_field   = StructField(name=\"summary\",   dataType=StringType(), nullable=True)\n",
    "content_field   = StructField(name=\"content\",   dataType=StringType(), nullable=True)\n",
    "body_field      = StructField(name=\"body\",      dataType=StringType(), nullable=True)\n",
    "\n",
    "schema_v0 = StructType([subreddit_field])\n",
    "schema_v1 = StructType([subreddit_field, summary_field, content_field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd171298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_50k.json, run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_50k.json, run 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_50k.json, run 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_50k.json, run 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_50k.json, run 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_100k.json, run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_100k.json, run 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_100k.json, run 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_100k.json, run 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_100k.json, run 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_200k.json, run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_200k.json, run 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_200k.json, run 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_200k.json, run 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_200k.json, run 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_500k.json, run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_500k.json, run 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_500k.json, run 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_500k.json, run 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_500k.json, run 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/corpus-webis-tldr-17.json, run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/corpus-webis-tldr-17.json, run 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/corpus-webis-tldr-17.json, run 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/corpus-webis-tldr-17.json, run 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/corpus-webis-tldr-17.json, run 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   File  File size Executor cores Executor memory Max cores  \\\n",
      "0            reddit_50k   0.365163              2              2g         2   \n",
      "1           reddit_100k   0.730300              4              4g         4   \n",
      "2           reddit_200k   1.456891              4              4g         4   \n",
      "3           reddit_500k   3.637079              4              4g        10   \n",
      "4  corpus-webis-tldr-17  18.272816              4              4g        32   \n",
      "\n",
      "   Data load time  Training time  Evaluation time  Overall exec time  \\\n",
      "0       10.302017      45.125135         3.271335          59.005185   \n",
      "1       10.082480      29.313975         3.010626          42.592113   \n",
      "2       13.161324      43.896724         4.674155          61.997070   \n",
      "3       19.901318     180.384358        13.887223         214.570157   \n",
      "4       88.463843     481.229342        28.389683         598.375960   \n",
      "\n",
      "   Model accuracy  \n",
      "0        0.480953  \n",
      "1        0.500373  \n",
      "2        0.526313  \n",
      "3        0.539488  \n",
      "4        0.550271  \n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "hdfs_path = \"hdfs://192.168.2.156:9000\"\n",
    "\n",
    "file_path = \"/data/reddit/\"\n",
    "\n",
    "files = [\"reddit_50k.json\", \"reddit_100k.json\", \n",
    "         \"reddit_200k.json\", \"reddit_500k.json\", \n",
    "         \"corpus-webis-tldr-17.json\"]\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        res = evaluate_performance(hdfs_path, f\"{file_path}{file}\", schema=schema_v1, n=5)\n",
    "        results.append([file[:-5]] + res)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Crashed when evaluating {file} with error:\")\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(results, columns=['File', 'File size', 'Executor cores', 'Executor memory', 'Max cores', 'Data load time', 'Training time', 'Evaluation time', 'Overall exec time', 'Model accuracy']) \n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2eaa008",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/home/ubuntu/out/\"\n",
    "out_name = \"log_reg_performance_data\"\n",
    "result_df.to_csv(path_or_buf=f\"{out_path}{out_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31a462cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in ./.local/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in ./.local/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl \n",
    "with pd.ExcelWriter(f\"{out_path}{out_name}.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    result_df.to_excel(writer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7ba16-375f-4236-bbee-1776bc00f0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
