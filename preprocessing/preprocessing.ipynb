{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb861e81-8564-4107-82cf-b14a07c4834a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Setting up Spark Session / Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8945a531-2b34-46d6-b30f-517765260936",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/09 21:47:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.156:7077\") \\\n",
    "        .appName(\"Project Group 32\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"220s\")\\\n",
    "        .config(\"spark.executor.cores\", 4)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d29bd-bfda-477c-be1e-c3852e65deec",
   "metadata": {},
   "source": [
    "## Loading the data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26cf81e-66b1-4e10-a6c2-c8761e614e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "��{\u0000\"\u0000a\u0000u\u0000t\u0000h\u0000o\u0000r\u0000\"\u0000:\u0000\"\u0000r\u0000a\u0000y\u0000s\u0000o\u0000f\u0000d\u0000a\u0000r\u0000k\u0000m\u0000a\u0000t\u0000t\u0000e\u0000r\u0000\"\u0000,\u0000\"\u0000b\u0000o\u0000d\u0000y\u0000\"\u0000:\u0000\"\u0000I\u0000 \u0000t\u0000h\u0000i\u0000n\u0000k\u0000 \u0000i\u0000t\u0000 \u0000s\u0000h\u0000o\u0000u\u0000l\u0000d\u0000 \u0000b\u0000e\u0000 \u0000f\u0000i\u0000x\u0000e\u0000d\u0000 \u0000o\u0000n\u0000 \u0000e\u0000i\u0000t\u0000h\u0000e\u0000r\u0000 \u0000U\u0000T\u0000C\u0000 \u0000s\u0000t\u0000a\u0000n\u0000d\u0000a\u0000r\u0000d\u0000 \u0000o\u0000r\u0000 \u0000U\u0000T\u0000C\u0000+\u00001\u0000 \u0000y\u0000e\u0000a\u0000r\u0000 \u0000a\u0000r\u0000o\u0000u\u0000n\u0000d\u0000,\u0000 \u0000w\u0000i\u0000t\u0000h\u0000 \u0000t\u0000h\u0000e\u0000 \u0000c\u0000u\u0000r\u0000r\u0000e\u0000n\u0000t\u0000 \u0000z\u0000o\u0000n\u0000e\u0000 \u0000o\u0000f\u0000f\u0000s\u0000e\u0000t\u0000s\u0000.\u0000\\\u0000n\u0000\\\u0000n\u0000M\u0000o\u0000v\u0000i\u0000n\u0000g\u0000 \u0000t\u0000i\u0000m\u0000e\u0000s\u0000c\u0000a\u0000l\u0000e\u0000s\u0000 \u0000a\u0000d\u0000d\u0000 \u0000a\u0000 \u0000l\u0000o\u0000t\u0000 \u0000o\u0000f\u0000 \u0000c\u0000o\u0000m\u0000p\u0000l\u0000e\u0000x\u0000i\u0000t\u0000y\u0000 \u0000t\u0000o\u0000 \u0000t\u0000h\u0000e\u0000 \u0000i\u0000m\u0000p\u0000l\u0000e\u0000m\u0000e\u0000n\u0000t\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000o\u0000f\u0000 \u0000t\u0000i\u0000m\u0000e\u0000k\u0000e\u0000e\u0000p\u0000i\u0000n\u0000g\u0000 \u0000s\u0000y\u0000s\u0000t\u0000e\u0000m\u0000s\u0000 \u0000a\u0000n\u0000d\u0000 \u0000h\u0000a\u0000v\u0000e\u0000 \u0000[\u0000d\u0000u\u0000b\u0000i\u0000o\u0000u\u0000s\u0000 \u0000v\u0000a\u0000l\u0000u\u0000e\u0000]\u0000(\u0000 \u0000\\\u0000n\u0000\\\u0000n\u0000I\u0000 \u0000t\u0000h\u0000i\u0000n\u0000k\u0000 \u0000s\u0000e\u0000a\u0000s\u0000o\u0000n\u0000a\u0000l\u0000 \u0000s\u0000h\u0000i\u0000f\u0000t\u0000i\u0000n\u0000g\u0000 \u0000t\u0000i\u0000m\u0000e\u0000 \u0000m\u0000a\u0000d\u0000e\u0000 \u0000s\u0000e\u0000n\u0000s\u0000e\u0000 \u0000i\u0000n\u0000 \u0000t\u0000h\u0000e\u0000 \u0000p\u0000r\u0000e\u0000-\u0000e\u0000l\u0000e\u0000c\u0000t\u0000r\u0000i\u0000c\u0000 \u0000p\u0000a\u0000s\u0000t\u0000,\u0000 \u0000w\u0000h\u0000e\u0000n\u0000 \u0000t\u0000i\u0000m\u0000e\u0000k\u0000e\u0000e\u0000p\u0000i\u0000n\u0000g\u0000 \u0000w\u0000a\u0000s\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000f\u0000l\u0000e\u0000x\u0000i\u0000b\u0000l\u0000e\u0000 \u0000a\u0000n\u0000d\u0000 \u0000a\u0000r\u0000t\u0000i\u0000f\u0000i\u0000c\u0000i\u0000a\u0000l\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000 \u0000w\u0000a\u0000s\u0000 \u0000i\u0000n\u0000e\u0000f\u0000f\u0000i\u0000c\u0000i\u0000e\u0000n\u0000t\u0000 \u0000a\u0000n\u0000d\u0000 \u0000o\u0000f\u0000t\u0000e\u0000n\u0000 \u0000d\u0000a\u0000n\u0000g\u0000e\u0000r\u0000o\u0000u\u0000s\u0000.\u0000 \u0000\\\u0000n\u0000\\\u0000n\u0000N\u0000o\u0000w\u0000 \u0000w\u0000e\u0000 \u0000h\u0000a\u0000v\u0000e\u0000 \u0000m\u0000a\u0000c\u0000h\u0000i\u0000n\u0000e\u0000s\u0000 \u0000t\u0000h\u0000a\u0000t\u0000 \u0000w\u0000o\u0000r\u0000k\u0000 \u0000e\u0000a\u0000s\u0000i\u0000l\u0000y\u0000 \u0000w\u0000i\u0000t\u0000h\u0000 \u0000s\u0000i\u0000m\u0000p\u0000l\u0000e\u0000 \u0000t\u0000i\u0000m\u0000e\u0000k\u0000e\u0000e\u0000p\u0000i\u0000n\u0000g\u0000 \u0000r\u0000u\u0000l\u0000e\u0000s\u0000,\u0000 \u0000a\u0000n\u0000d\u0000 \u0000i\u0000t\u0000'\u0000s\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000b\u0000e\u0000n\u0000e\u0000f\u0000i\u0000c\u0000i\u0000a\u0000l\u0000 \u0000t\u0000o\u0000 \u0000s\u0000p\u0000e\u0000n\u0000d\u0000 \u0000a\u0000 \u0000s\u0000m\u0000a\u0000l\u0000l\u0000 \u0000a\u0000m\u0000o\u0000u\u0000n\u0000t\u0000 \u0000o\u0000n\u0000 \u0000e\u0000n\u0000e\u0000r\u0000g\u0000y\u0000 \u0000f\u0000o\u0000r\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000i\u0000n\u0000g\u0000,\u0000 \u0000a\u0000n\u0000d\u0000 \u0000s\u0000a\u0000v\u0000e\u0000 \u0000t\u0000h\u0000e\u0000 \u0000l\u0000a\u0000r\u0000g\u0000e\u0000r\u0000 \u0000c\u0000o\u0000s\u0000t\u0000 \u0000o\u0000f\u0000 \u0000e\u0000n\u0000g\u0000i\u0000n\u0000e\u0000e\u0000r\u0000i\u0000n\u0000g\u0000 \u0000t\u0000h\u0000i\u0000n\u0000g\u0000s\u0000 \u0000t\u0000o\u0000 \u0000w\u0000o\u0000r\u0000k\u0000 \u0000w\u0000i\u0000t\u0000h\u0000 \u0000t\u0000h\u0000e\u0000 \u0000c\u0000o\u0000m\u0000p\u0000l\u0000e\u0000x\u0000 \u0000t\u0000i\u0000m\u0000e\u0000k\u0000e\u0000e\u0000p\u0000i\u0000n\u0000g\u0000 \u0000r\u0000u\u0000l\u0000e\u0000s\u0000,\u0000 \u0000a\u0000s\u0000 \u0000w\u0000e\u0000l\u0000l\u0000 \u0000a\u0000s\u0000 \u0000s\u0000a\u0000v\u0000i\u0000n\u0000g\u0000 \u0000t\u0000h\u0000e\u0000 \u0000i\u0000r\u0000r\u0000i\u0000t\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000t\u0000o\u0000 \u0000h\u0000u\u0000m\u0000a\u0000n\u0000s\u0000.\u0000\\\u0000n\u0000\\\u0000n\u0000L\u0000i\u0000g\u0000h\u0000t\u0000i\u0000n\u0000g\u0000 \u0000h\u0000a\u0000s\u0000 \u0000g\u0000o\u0000t\u0000t\u0000e\u0000n\u0000 \u0000m\u0000u\u0000c\u0000h\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000e\u0000f\u0000f\u0000i\u0000c\u0000i\u0000e\u0000n\u0000t\u0000 \u0000o\u0000v\u0000e\u0000r\u0000 \u0000t\u0000i\u0000m\u0000e\u0000;\u0000 \u0000w\u0000e\u0000 \u0000c\u0000a\u0000n\u0000 \u0000s\u0000q\u0000u\u0000e\u0000e\u0000z\u0000e\u0000 \u0000o\u0000u\u0000t\u0000 \u0000a\u0000 \u0000l\u0000o\u0000t\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000p\u0000h\u0000o\u0000t\u0000o\u0000n\u0000s\u0000 \u0000p\u0000e\u0000r\u0000 \u0000u\u0000n\u0000i\u0000t\u0000 \u0000o\u0000f\u0000 \u0000e\u0000n\u0000e\u0000r\u0000g\u0000y\u0000 \u0000f\u0000r\u0000o\u0000m\u0000 \u0000a\u0000 \u00002\u00000\u00001\u00002\u0000 \u0000C\u0000F\u0000L\u0000 \u0000o\u0000r\u0000 \u0000L\u0000E\u0000D\u0000 \u0000t\u0000h\u0000a\u0000n\u0000 \u0000a\u0000 \u0000c\u0000a\u0000n\u0000d\u0000l\u0000e\u0000 \u0000c\u0000o\u0000u\u0000l\u0000d\u0000 \u0000i\u0000n\u0000 \u00001\u00007\u00008\u00000\u0000,\u0000 \u0000o\u0000r\u0000 \u0000a\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000b\u0000u\u0000l\u0000b\u0000 \u0000c\u0000o\u0000u\u0000l\u0000d\u0000 \u0000i\u0000n\u0000 \u00001\u00009\u00005\u00000\u0000.\u0000 \u0000\\\u0000n\u0000\\\u0000n\u0000T\u0000h\u0000e\u0000r\u0000e\u0000'\u0000s\u0000 \u0000a\u0000 \u0000l\u0000o\u0000t\u0000 \u0000o\u0000f\u0000 \u0000r\u0000o\u0000o\u0000m\u0000 \u0000f\u0000o\u0000r\u0000 \u0000i\u0000m\u0000p\u0000r\u0000o\u0000v\u0000e\u0000m\u0000e\u0000n\u0000t\u0000 \u0000i\u0000n\u0000 \u0000h\u0000o\u0000w\u0000 \u0000w\u0000e\u0000 \u0000u\u0000s\u0000e\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000s\u0000 \u0000a\u0000s\u0000 \u0000w\u0000e\u0000l\u0000l\u0000;\u0000 \u0000a\u0000s\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000i\u0000n\u0000g\u0000 \u0000c\u0000o\u0000n\u0000t\u0000r\u0000o\u0000l\u0000 \u0000g\u0000e\u0000t\u0000s\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000i\u0000n\u0000t\u0000e\u0000l\u0000l\u0000i\u0000g\u0000e\u0000n\u0000t\u0000,\u0000 \u0000t\u0000h\u0000e\u0000r\u0000e\u0000 \u0000w\u0000i\u0000l\u0000l\u0000 \u0000b\u0000e\u0000 \u0000a\u0000 \u0000l\u0000o\u0000t\u0000 \u0000o\u0000f\u0000 \u0000s\u0000a\u0000v\u0000i\u0000n\u0000g\u0000s\u0000 \u0000f\u0000r\u0000o\u0000m\u0000 \u0000n\u0000o\u0000t\u0000 \u0000i\u0000l\u0000l\u0000u\u0000m\u0000i\u0000n\u0000a\u0000t\u0000i\u0000n\u0000g\u0000 \u0000i\u0000n\u0000a\u0000c\u0000t\u0000i\u0000v\u0000e\u0000 \u0000s\u0000p\u0000a\u0000c\u0000e\u0000s\u0000 \u0000c\u0000o\u0000n\u0000s\u0000t\u0000a\u0000n\u0000t\u0000l\u0000y\u0000.\u0000\\\u0000n\u0000\\\u0000n\u0000t\u0000l\u0000;\u0000d\u0000r\u0000:\u0000 \u0000S\u0000h\u0000i\u0000f\u0000t\u0000i\u0000n\u0000g\u0000 \u0000s\u0000e\u0000a\u0000s\u0000o\u0000n\u0000a\u0000l\u0000 \u0000t\u0000i\u0000m\u0000e\u0000 \u0000i\u0000s\u0000 \u0000n\u0000o\u0000 \u0000l\u0000o\u0000n\u0000g\u0000e\u0000r\u0000 \u0000w\u0000o\u0000r\u0000t\u0000h\u0000 \u0000i\u0000t\u0000.\u0000\"\u0000,\u0000\"\u0000n\u0000o\u0000r\u0000m\u0000a\u0000l\u0000i\u0000z\u0000e\u0000d\u0000B\u0000o\u0000d\u0000y\u0000\"\u0000:\u0000\"\u0000I\u0000 \u0000t\u0000h\u0000i\u0000n\u0000k\u0000 \u0000i\u0000t\u0000 \u0000s\u0000h\u0000o\u0000u\u0000l\u0000d\u0000 \u0000b\u0000e\u0000 \u0000f\u0000i\u0000x\u0000e\u0000d\u0000 \u0000o\u0000n\u0000 \u0000e\u0000i\u0000t\u0000h\u0000e\u0000r\u0000 \u0000U\u0000T\u0000C\u0000 \u0000s\u0000t\u0000a\u0000n\u0000d\u0000a\u0000r\u0000d\u0000 \u0000o\u0000r\u0000 \u0000U\u0000T\u0000C\u0000+\u00001\u0000 \u0000y\u0000e\u0000a\u0000r\u0000 \u0000a\u0000r\u0000o\u0000u\u0000n\u0000d\u0000,\u0000 \u0000w\u0000i\u0000t\u0000h\u0000 \u0000t\u0000h\u0000e\u0000 \u0000c\u0000u\u0000r\u0000r\u0000e\u0000n\u0000t\u0000 \u0000z\u0000o\u0000n\u0000e\u0000 \u0000o\u0000f\u0000f\u0000s\u0000e\u0000t\u0000s\u0000.\u0000 \u0000\\\u0000n\u0000 \u0000M\u0000o\u0000v\u0000i\u0000n\u0000g\u0000 \u0000t\u0000i\u0000m\u0000e\u0000s\u0000c\u0000a\u0000l\u0000e\u0000s\u0000 \u0000a\u0000d\u0000d\u0000 \u0000a\u0000 \u0000l\u0000o\u0000t\u0000 \u0000o\u0000f\u0000 \u0000c\u0000o\u0000m\u0000p\u0000l\u0000e\u0000x\u0000i\u0000t\u0000y\u0000 \u0000t\u0000o\u0000 \u0000t\u0000h\u0000e\u0000 \u0000i\u0000m\u0000p\u0000l\u0000e\u0000m\u0000e\u0000n\u0000t\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000o\u0000f\u0000 \u0000t\u0000i\u0000m\u0000e\u0000k\u0000e\u0000e\u0000p\u0000i\u0000n\u0000g\u0000 \u0000s\u0000y\u0000s\u0000t\u0000e\u0000m\u0000s\u0000 \u0000a\u0000n\u0000d\u0000 \u0000h\u0000a\u0000v\u0000e\u0000 \u0000[\u0000d\u0000u\u0000b\u0000i\u0000o\u0000u\u0000s\u0000 \u0000v\u0000a\u0000l\u0000u\u0000e\u0000]\u0000(\u0000 \u0000\\\u0000n\u0000 \u0000I\u0000 \u0000t\u0000h\u0000i\u0000n\u0000k\u0000 \u0000s\u0000e\u0000a\u0000s\u0000o\u0000n\u0000a\u0000l\u0000 \u0000s\u0000h\u0000i\u0000f\u0000t\u0000i\u0000n\u0000g\u0000 \u0000t\u0000i\u0000m\u0000e\u0000 \u0000m\u0000a\u0000d\u0000e\u0000 \u0000s\u0000e\u0000n\u0000s\u0000e\u0000 \u0000i\u0000n\u0000 \u0000t\u0000h\u0000e\u0000 \u0000p\u0000r\u0000e\u0000-\u0000e\u0000l\u0000e\u0000c\u0000t\u0000r\u0000i\u0000c\u0000 \u0000p\u0000a\u0000s\u0000t\u0000,\u0000 \u0000w\u0000h\u0000e\u0000n\u0000 \u0000t\u0000i\u0000m\u0000e\u0000k\u0000e\u0000e\u0000p\u0000i\u0000n\u0000g\u0000 \u0000w\u0000a\u0000s\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000f\u0000l\u0000e\u0000x\u0000i\u0000b\u0000l\u0000e\u0000 \u0000a\u0000n\u0000d\u0000 \u0000a\u0000r\u0000t\u0000i\u0000f\u0000i\u0000c\u0000i\u0000a\u0000l\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000 \u0000w\u0000a\u0000s\u0000 \u0000i\u0000n\u0000e\u0000f\u0000f\u0000i\u0000c\u0000i\u0000e\u0000n\u0000t\u0000 \u0000a\u0000n\u0000d\u0000 \u0000o\u0000f\u0000t\u0000e\u0000n\u0000 \u0000d\u0000a\u0000n\u0000g\u0000e\u0000r\u0000o\u0000u\u0000s\u0000.\u0000 \u0000\\\u0000n\u0000 \u0000N\u0000o\u0000w\u0000 \u0000w\u0000e\u0000 \u0000h\u0000a\u0000v\u0000e\u0000 \u0000m\u0000a\u0000c\u0000h\u0000i\u0000n\u0000e\u0000s\u0000 \u0000t\u0000h\u0000a\u0000t\u0000 \u0000w\u0000o\u0000r\u0000k\u0000 \u0000e\u0000a\u0000s\u0000i\u0000l\u0000y\u0000 \u0000w\u0000i\u0000t\u0000h\u0000 \u0000s\u0000i\u0000m\u0000p\u0000l\u0000e\u0000 \u0000t\u0000i\u0000m\u0000e\u0000k\u0000e\u0000e\u0000p\u0000i\u0000n\u0000g\u0000 \u0000r\u0000u\u0000l\u0000e\u0000s\u0000,\u0000 \u0000a\u0000n\u0000d\u0000 \u0000i\u0000t\u0000'\u0000s\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000b\u0000e\u0000n\u0000e\u0000f\u0000i\u0000c\u0000i\u0000a\u0000l\u0000 \u0000t\u0000o\u0000 \u0000s\u0000p\u0000e\u0000n\u0000d\u0000 \u0000a\u0000 \u0000s\u0000m\u0000a\u0000l\u0000l\u0000 \u0000a\u0000m\u0000o\u0000u\u0000n\u0000t\u0000 \u0000o\u0000n\u0000 \u0000e\u0000n\u0000e\u0000r\u0000g\u0000y\u0000 \u0000f\u0000o\u0000r\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000i\u0000n\u0000g\u0000,\u0000 \u0000a\u0000n\u0000d\u0000 \u0000s\u0000a\u0000v\u0000e\u0000 \u0000t\u0000h\u0000e\u0000 \u0000l\u0000a\u0000r\u0000g\u0000e\u0000r\u0000 \u0000c\u0000o\u0000s\u0000t\u0000 \u0000o\u0000f\u0000 \u0000e\u0000n\u0000g\u0000i\u0000n\u0000e\u0000e\u0000r\u0000i\u0000n\u0000g\u0000 \u0000t\u0000h\u0000i\u0000n\u0000g\u0000s\u0000 \u0000t\u0000o\u0000 \u0000w\u0000o\u0000r\u0000k\u0000 \u0000w\u0000i\u0000t\u0000h\u0000 \u0000t\u0000h\u0000e\u0000 \u0000c\u0000o\u0000m\u0000p\u0000l\u0000e\u0000x\u0000 \u0000t\u0000i\u0000m\u0000e\u0000k\u0000e\u0000e\u0000p\u0000i\u0000n\u0000g\u0000 \u0000r\u0000u\u0000l\u0000e\u0000s\u0000,\u0000 \u0000a\u0000s\u0000 \u0000w\u0000e\u0000l\u0000l\u0000 \u0000a\u0000s\u0000 \u0000s\u0000a\u0000v\u0000i\u0000n\u0000g\u0000 \u0000t\u0000h\u0000e\u0000 \u0000i\u0000r\u0000r\u0000i\u0000t\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000t\u0000o\u0000 \u0000h\u0000u\u0000m\u0000a\u0000n\u0000s\u0000.\u0000 \u0000\\\u0000n\u0000 \u0000L\u0000i\u0000g\u0000h\u0000t\u0000i\u0000n\u0000g\u0000 \u0000h\u0000a\u0000s\u0000 \u0000g\u0000o\u0000t\u0000t\u0000e\u0000n\u0000 \u0000m\u0000u\u0000c\u0000h\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000e\u0000f\u0000f\u0000i\u0000c\u0000i\u0000e\u0000n\u0000t\u0000 \u0000o\u0000v\u0000e\u0000r\u0000 \u0000t\u0000i\u0000m\u0000e\u0000;\u0000 \u0000w\u0000e\u0000 \u0000c\u0000a\u0000n\u0000 \u0000s\u0000q\u0000u\u0000e\u0000e\u0000z\u0000e\u0000 \u0000o\u0000u\u0000t\u0000 \u0000a\u0000 \u0000l\u0000o\u0000t\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000p\u0000h\u0000o\u0000t\u0000o\u0000n\u0000s\u0000 \u0000p\u0000e\u0000r\u0000 \u0000u\u0000n\u0000i\u0000t\u0000 \u0000o\u0000f\u0000 \u0000e\u0000n\u0000e\u0000r\u0000g\u0000y\u0000 \u0000f\u0000r\u0000o\u0000m\u0000 \u0000a\u0000 \u00002\u00000\u00001\u00002\u0000 \u0000C\u0000F\u0000L\u0000 \u0000o\u0000r\u0000 \u0000L\u0000E\u0000D\u0000 \u0000t\u0000h\u0000a\u0000n\u0000 \u0000a\u0000 \u0000c\u0000a\u0000n\u0000d\u0000l\u0000e\u0000 \u0000c\u0000o\u0000u\u0000l\u0000d\u0000 \u0000i\u0000n\u0000 \u00001\u00007\u00008\u00000\u0000,\u0000 \u0000o\u0000r\u0000 \u0000a\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000b\u0000u\u0000l\u0000b\u0000 \u0000c\u0000o\u0000u\u0000l\u0000d\u0000 \u0000i\u0000n\u0000 \u00001\u00009\u00005\u00000\u0000.\u0000 \u0000\\\u0000n\u0000 \u0000T\u0000h\u0000e\u0000r\u0000e\u0000'\u0000s\u0000 \u0000a\u0000 \u0000l\u0000o\u0000t\u0000 \u0000o\u0000f\u0000 \u0000r\u0000o\u0000o\u0000m\u0000 \u0000f\u0000o\u0000r\u0000 \u0000i\u0000m\u0000p\u0000r\u0000o\u0000v\u0000e\u0000m\u0000e\u0000n\u0000t\u0000 \u0000i\u0000n\u0000 \u0000h\u0000o\u0000w\u0000 \u0000w\u0000e\u0000 \u0000u\u0000s\u0000e\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000s\u0000 \u0000a\u0000s\u0000 \u0000w\u0000e\u0000l\u0000l\u0000;\u0000 \u0000a\u0000s\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000i\u0000n\u0000g\u0000 \u0000c\u0000o\u0000n\u0000t\u0000r\u0000o\u0000l\u0000 \u0000g\u0000e\u0000t\u0000s\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000i\u0000n\u0000t\u0000e\u0000l\u0000l\u0000i\u0000g\u0000e\u0000n\u0000t\u0000,\u0000 \u0000t\u0000h\u0000e\u0000r\u0000e\u0000 \u0000w\u0000i\u0000l\u0000l\u0000 \u0000b\u0000e\u0000 \u0000a\u0000 \u0000l\u0000o\u0000t\u0000 \u0000o\u0000f\u0000 \u0000s\u0000a\u0000v\u0000i\u0000n\u0000g\u0000s\u0000 \u0000f\u0000r\u0000o\u0000m\u0000 \u0000n\u0000o\u0000t\u0000 \u0000i\u0000l\u0000l\u0000u\u0000m\u0000i\u0000n\u0000a\u0000t\u0000i\u0000n\u0000g\u0000 \u0000i\u0000n\u0000a\u0000c\u0000t\u0000i\u0000v\u0000e\u0000 \u0000s\u0000p\u0000a\u0000c\u0000e\u0000s\u0000 \u0000c\u0000o\u0000n\u0000s\u0000t\u0000a\u0000n\u0000t\u0000l\u0000y\u0000.\u0000 \u0000\\\u0000n\u0000 \u0000t\u0000l\u0000;\u0000d\u0000r\u0000:\u0000 \u0000S\u0000h\u0000i\u0000f\u0000t\u0000i\u0000n\u0000g\u0000 \u0000s\u0000e\u0000a\u0000s\u0000o\u0000n\u0000a\u0000l\u0000 \u0000t\u0000i\u0000m\u0000e\u0000 \u0000i\u0000s\u0000 \u0000n\u0000o\u0000 \u0000l\u0000o\u0000n\u0000g\u0000e\u0000r\u0000 \u0000w\u0000o\u0000r\u0000t\u0000h\u0000 \u0000i\u0000t\u0000.\u0000 \u0000\\\u0000n\u0000\"\u0000,\u0000\"\u0000c\u0000o\u0000n\u0000t\u0000e\u0000n\u0000t\u0000\"\u0000:\u0000\"\u0000I\u0000 \u0000t\u0000h\u0000i\u0000n\u0000k\u0000 \u0000i\u0000t\u0000 \u0000s\u0000h\u0000o\u0000u\u0000l\u0000d\u0000 \u0000b\u0000e\u0000 \u0000f\u0000i\u0000x\u0000e\u0000d\u0000 \u0000o\u0000n\u0000 \u0000e\u0000i\u0000t\u0000h\u0000e\u0000r\u0000 \u0000U\u0000T\u0000C\u0000 \u0000s\u0000t\u0000a\u0000n\u0000d\u0000a\u0000r\u0000d\u0000 \u0000o\u0000r\u0000 \u0000U\u0000T\u0000C\u0000+\u00001\u0000 \u0000y\u0000e\u0000a\u0000r\u0000 \u0000a\u0000r\u0000o\u0000u\u0000n\u0000d\u0000,\u0000 \u0000w\u0000i\u0000t\u0000h\u0000 \u0000t\u0000h\u0000e\u0000 \u0000c\u0000u\u0000r\u0000r\u0000e\u0000n\u0000t\u0000 \u0000z\u0000o\u0000n\u0000e\u0000 \u0000o\u0000f\u0000f\u0000s\u0000e\u0000t\u0000s\u0000.\u0000 \u0000\\\u0000n\u0000 \u0000M\u0000o\u0000v\u0000i\u0000n\u0000g\u0000 \u0000t\u0000i\u0000m\u0000e\u0000s\u0000c\u0000a\u0000l\u0000e\u0000s\u0000 \u0000a\u0000d\u0000d\u0000 \u0000a\u0000 \u0000l\u0000o\u0000t\u0000 \u0000o\u0000f\u0000 \u0000c\u0000o\u0000m\u0000p\u0000l\u0000e\u0000x\u0000i\u0000t\u0000y\u0000 \u0000t\u0000o\u0000 \u0000t\u0000h\u0000e\u0000 \u0000i\u0000m\u0000p\u0000l\u0000e\u0000m\u0000e\u0000n\u0000t\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000o\u0000f\u0000 \u0000t\u0000i\u0000m\u0000e\u0000k\u0000e\u0000e\u0000p\u0000i\u0000n\u0000g\u0000 \u0000s\u0000y\u0000s\u0000t\u0000e\u0000m\u0000s\u0000 \u0000a\u0000n\u0000d\u0000 \u0000h\u0000a\u0000v\u0000e\u0000 \u0000[\u0000d\u0000u\u0000b\u0000i\u0000o\u0000u\u0000s\u0000 \u0000v\u0000a\u0000l\u0000u\u0000e\u0000]\u0000(\u0000 \u0000\\\u0000n\u0000 \u0000I\u0000 \u0000t\u0000h\u0000i\u0000n\u0000k\u0000 \u0000s\u0000e\u0000a\u0000s\u0000o\u0000n\u0000a\u0000l\u0000 \u0000s\u0000h\u0000i\u0000f\u0000t\u0000i\u0000n\u0000g\u0000 \u0000t\u0000i\u0000m\u0000e\u0000 \u0000m\u0000a\u0000d\u0000e\u0000 \u0000s\u0000e\u0000n\u0000s\u0000e\u0000 \u0000i\u0000n\u0000 \u0000t\u0000h\u0000e\u0000 \u0000p\u0000r\u0000e\u0000-\u0000e\u0000l\u0000e\u0000c\u0000t\u0000r\u0000i\u0000c\u0000 \u0000p\u0000a\u0000s\u0000t\u0000,\u0000 \u0000w\u0000h\u0000e\u0000n\u0000 \u0000t\u0000i\u0000m\u0000e\u0000k\u0000e\u0000e\u0000p\u0000i\u0000n\u0000g\u0000 \u0000w\u0000a\u0000s\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000f\u0000l\u0000e\u0000x\u0000i\u0000b\u0000l\u0000e\u0000 \u0000a\u0000n\u0000d\u0000 \u0000a\u0000r\u0000t\u0000i\u0000f\u0000i\u0000c\u0000i\u0000a\u0000l\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000 \u0000w\u0000a\u0000s\u0000 \u0000i\u0000n\u0000e\u0000f\u0000f\u0000i\u0000c\u0000i\u0000e\u0000n\u0000t\u0000 \u0000a\u0000n\u0000d\u0000 \u0000o\u0000f\u0000t\u0000e\u0000n\u0000 \u0000d\u0000a\u0000n\u0000g\u0000e\u0000r\u0000o\u0000u\u0000s\u0000.\u0000 \u0000\\\u0000n\u0000 \u0000N\u0000o\u0000w\u0000 \u0000w\u0000e\u0000 \u0000h\u0000a\u0000v\u0000e\u0000 \u0000m\u0000a\u0000c\u0000h\u0000i\u0000n\u0000e\u0000s\u0000 \u0000t\u0000h\u0000a\u0000t\u0000 \u0000w\u0000o\u0000r\u0000k\u0000 \u0000e\u0000a\u0000s\u0000i\u0000l\u0000y\u0000 \u0000w\u0000i\u0000t\u0000h\u0000 \u0000s\u0000i\u0000m\u0000p\u0000l\u0000e\u0000 \u0000t\u0000i\u0000m\u0000e\u0000k\u0000e\u0000e\u0000p\u0000i\u0000n\u0000g\u0000 \u0000r\u0000u\u0000l\u0000e\u0000s\u0000,\u0000 \u0000a\u0000n\u0000d\u0000 \u0000i\u0000t\u0000'\u0000s\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000b\u0000e\u0000n\u0000e\u0000f\u0000i\u0000c\u0000i\u0000a\u0000l\u0000 \u0000t\u0000o\u0000 \u0000s\u0000p\u0000e\u0000n\u0000d\u0000 \u0000a\u0000 \u0000s\u0000m\u0000a\u0000l\u0000l\u0000 \u0000a\u0000m\u0000o\u0000u\u0000n\u0000t\u0000 \u0000o\u0000n\u0000 \u0000e\u0000n\u0000e\u0000r\u0000g\u0000y\u0000 \u0000f\u0000o\u0000r\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000i\u0000n\u0000g\u0000,\u0000 \u0000a\u0000n\u0000d\u0000 \u0000s\u0000a\u0000v\u0000e\u0000 \u0000t\u0000h\u0000e\u0000 \u0000l\u0000a\u0000r\u0000g\u0000e\u0000r\u0000 \u0000c\u0000o\u0000s\u0000t\u0000 \u0000o\u0000f\u0000 \u0000e\u0000n\u0000g\u0000i\u0000n\u0000e\u0000e\u0000r\u0000i\u0000n\u0000g\u0000 \u0000t\u0000h\u0000i\u0000n\u0000g\u0000s\u0000 \u0000t\u0000o\u0000 \u0000w\u0000o\u0000r\u0000k\u0000 \u0000w\u0000i\u0000t\u0000h\u0000 \u0000t\u0000h\u0000e\u0000 \u0000c\u0000o\u0000m\u0000p\u0000l\u0000e\u0000x\u0000 \u0000t\u0000i\u0000m\u0000e\u0000k\u0000e\u0000e\u0000p\u0000i\u0000n\u0000g\u0000 \u0000r\u0000u\u0000l\u0000e\u0000s\u0000,\u0000 \u0000a\u0000s\u0000 \u0000w\u0000e\u0000l\u0000l\u0000 \u0000a\u0000s\u0000 \u0000s\u0000a\u0000v\u0000i\u0000n\u0000g\u0000 \u0000t\u0000h\u0000e\u0000 \u0000i\u0000r\u0000r\u0000i\u0000t\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000t\u0000o\u0000 \u0000h\u0000u\u0000m\u0000a\u0000n\u0000s\u0000.\u0000 \u0000\\\u0000n\u0000 \u0000L\u0000i\u0000g\u0000h\u0000t\u0000i\u0000n\u0000g\u0000 \u0000h\u0000a\u0000s\u0000 \u0000g\u0000o\u0000t\u0000t\u0000e\u0000n\u0000 \u0000m\u0000u\u0000c\u0000h\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000e\u0000f\u0000f\u0000i\u0000c\u0000i\u0000e\u0000n\u0000t\u0000 \u0000o\u0000v\u0000e\u0000r\u0000 \u0000t\u0000i\u0000m\u0000e\u0000;\u0000 \u0000w\u0000e\u0000 \u0000c\u0000a\u0000n\u0000 \u0000s\u0000q\u0000u\u0000e\u0000e\u0000z\u0000e\u0000 \u0000o\u0000u\u0000t\u0000 \u0000a\u0000 \u0000l\u0000o\u0000t\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000p\u0000h\u0000o\u0000t\u0000o\u0000n\u0000s\u0000 \u0000p\u0000e\u0000r\u0000 \u0000u\u0000n\u0000i\u0000t\u0000 \u0000o\u0000f\u0000 \u0000e\u0000n\u0000e\u0000r\u0000g\u0000y\u0000 \u0000f\u0000r\u0000o\u0000m\u0000 \u0000a\u0000 \u00002\u00000\u00001\u00002\u0000 \u0000C\u0000F\u0000L\u0000 \u0000o\u0000r\u0000 \u0000L\u0000E\u0000D\u0000 \u0000t\u0000h\u0000a\u0000n\u0000 \u0000a\u0000 \u0000c\u0000a\u0000n\u0000d\u0000l\u0000e\u0000 \u0000c\u0000o\u0000u\u0000l\u0000d\u0000 \u0000i\u0000n\u0000 \u00001\u00007\u00008\u00000\u0000,\u0000 \u0000o\u0000r\u0000 \u0000a\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000b\u0000u\u0000l\u0000b\u0000 \u0000c\u0000o\u0000u\u0000l\u0000d\u0000 \u0000i\u0000n\u0000 \u00001\u00009\u00005\u00000\u0000.\u0000 \u0000\\\u0000n\u0000 \u0000T\u0000h\u0000e\u0000r\u0000e\u0000'\u0000s\u0000 \u0000a\u0000 \u0000l\u0000o\u0000t\u0000 \u0000o\u0000f\u0000 \u0000r\u0000o\u0000o\u0000m\u0000 \u0000f\u0000o\u0000r\u0000 \u0000i\u0000m\u0000p\u0000r\u0000o\u0000v\u0000e\u0000m\u0000e\u0000n\u0000t\u0000 \u0000i\u0000n\u0000 \u0000h\u0000o\u0000w\u0000 \u0000w\u0000e\u0000 \u0000u\u0000s\u0000e\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000s\u0000 \u0000a\u0000s\u0000 \u0000w\u0000e\u0000l\u0000l\u0000;\u0000 \u0000a\u0000s\u0000 \u0000l\u0000i\u0000g\u0000h\u0000t\u0000i\u0000n\u0000g\u0000 \u0000c\u0000o\u0000n\u0000t\u0000r\u0000o\u0000l\u0000 \u0000g\u0000e\u0000t\u0000s\u0000 \u0000m\u0000o\u0000r\u0000e\u0000 \u0000i\u0000n\u0000t\u0000e\u0000l\u0000l\u0000i\u0000g\u0000e\u0000n\u0000t\u0000,\u0000 \u0000t\u0000h\u0000e\u0000r\u0000e\u0000 \u0000w\u0000i\u0000l\u0000l\u0000 \u0000b\u0000e\u0000 \u0000a\u0000 \u0000l\u0000o\u0000t\u0000 \u0000o\u0000f\u0000 \u0000s\u0000a\u0000v\u0000i\u0000n\u0000g\u0000s\u0000 \u0000f\u0000r\u0000o\u0000m\u0000 \u0000n\u0000o\u0000t\u0000 \u0000i\u0000l\u0000l\u0000u\u0000m\u0000i\u0000n\u0000a\u0000t\u0000i\u0000n\u0000g\u0000 \u0000i\u0000n\u0000a\u0000c\u0000t\u0000i\u0000v\u0000e\u0000 \u0000s\u0000p\u0000a\u0000c\u0000e\u0000s\u0000 \u0000c\u0000o\u0000n\u0000s\u0000t\u0000a\u0000n\u0000t\u0000l\u0000y\u0000.\u0000\"\u0000,\u0000\"\u0000c\u0000o\u0000n\u0000t\u0000e\u0000n\u0000t\u0000_\u0000l\u0000e\u0000n\u0000\"\u0000:\u00001\u00007\u00008\u0000,\u0000\"\u0000s\u0000u\u0000m\u0000m\u0000a\u0000r\u0000y\u0000\"\u0000:\u0000\"\u0000S\u0000h\u0000i\u0000f\u0000t\u0000i\u0000n\u0000g\u0000 \u0000s\u0000e\u0000a\u0000s\u0000o\u0000n\u0000a\u0000l\u0000 \u0000t\u0000i\u0000m\u0000e\u0000 \u0000i\u0000s\u0000 \u0000n\u0000o\u0000 \u0000l\u0000o\u0000n\u0000g\u0000e\u0000r\u0000 \u0000w\u0000o\u0000r\u0000t\u0000h\u0000 \u0000i\u0000t\u0000.\u0000\"\u0000,\u0000\"\u0000s\u0000u\u0000m\u0000m\u0000a\u0000r\u0000y\u0000_\u0000l\u0000e\u0000n\u0000\"\u0000:\u00008\u0000,\u0000\"\u0000i\u0000d\u0000\"\u0000:\u0000\"\u0000c\u00006\u00009\u0000a\u0000l\u00003\u0000r\u0000\"\u0000,\u0000\"\u0000s\u0000u\u0000b\u0000r\u0000e\u0000d\u0000d\u0000i\u0000t\u0000\"\u0000:\u0000\"\u0000m\u0000a\u0000t\u0000h\u0000\"\u0000,\u0000\"\u0000s\u0000u\u0000b\u0000r\u0000e\u0000d\u0000d\u0000i\u0000t\u0000_\u0000i\u0000d\u0000\"\u0000:\u0000\"\u0000t\u00005\u0000_\u00002\u0000q\u0000h\u00000\u0000n\u0000\"\u0000}\u0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# The same example, this time using map and reduce from the Spark API, and loading the text file from HDFS.\n",
    "\n",
    "lines = spark_context.textFile(\"hdfs://192.168.2.156:9000/data/reddit/reddit_50k.json\")\n",
    "print(lines.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c388e54-f3ff-418e-865e-618c7194132f",
   "metadata": {},
   "source": [
    "## Create a dataframe to analyse the posts line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "955d8c13-cf33-45c4-87b9-471bba476da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- content_len: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- normalizedBody: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- summary_len: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "# spark = SparkSession.builder.appName(\"RedditJSONProcessing\").getOrCreate()\n",
    "\n",
    "# Load JSON file into a Spark DataFrame\n",
    "df = spark_session.read.json(\"hdfs://192.168.2.156:9000/data/reddit/reddit_500k.json\")\n",
    "\n",
    "# Show schema to understand the structure\n",
    "df.printSchema()\n",
    "\n",
    "# Show first few rows to inspect data\n",
    "# df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d529b0ec-7072-44c4-ad36-14a1417a1817",
   "metadata": {},
   "source": [
    "## Count Total Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "798b8416-232f-47f0-a884-1a78b5a2c475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=======================================================> (29 + 1) / 30]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Posts: 1000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(f\"Total Posts: {df.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682b100-d96f-4f0e-abe7-90d2a8bebb5c",
   "metadata": {},
   "source": [
    "## How many Subreddits do exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae10602-3066-4837-b87e-dbf81fae1425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Subreddits: 9610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "unique_subreddits = df.select(\"subreddit\").distinct().count()\n",
    "print(f\"Unique Subreddits: {unique_subreddits}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f20027-72e2-4b5d-b1df-ebcf71d061bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==========================================>             (24 + 8) / 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|subreddit        |count |\n",
      "+-----------------+------+\n",
      "|NULL             |501789|\n",
      "|AskReddit        |117305|\n",
      "|leagueoflegends  |12088 |\n",
      "|AdviceAnimals    |9413  |\n",
      "|funny            |8578  |\n",
      "|politics         |8005  |\n",
      "|gaming           |7911  |\n",
      "|pics             |7803  |\n",
      "|atheism          |6766  |\n",
      "|explainlikeimfive|5701  |\n",
      "+-----------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "df.groupBy(\"subreddit\").count().orderBy(col(\"count\").desc()).show(10, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccd847-fc1a-4d1b-a57b-9a23483100bb",
   "metadata": {},
   "source": [
    "-- We see that many post are not assigned to a Subreddit, since we want to train a Classification model, we delete the NULL post --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "010ef1f9-9c85-4b1a-9b05-83c2ab80919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:======================================================> (31 + 1) / 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Posts After Filtering: 498212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter out rows where subreddit is NULL\n",
    "df_filtered = df.filter(col(\"subreddit\").isNotNull())\n",
    "\n",
    "# Show first few rows after filtering\n",
    "# df_filtered.show(5, truncate=False)\n",
    "\n",
    "# Count remaining rows\n",
    "print(f\"Total Posts After Filtering: {df_filtered.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595011a3-7885-4dea-ad15-a6224a8abf27",
   "metadata": {},
   "source": [
    "## To prepare the Data for our ML Classification Model, we use the columns summary and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92ddbad7-e2f5-49a2-83a1-f59953cb1027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[subreddit: string, summary: string, content: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter out NULL subreddit, summary, or content\n",
    "df_filtered = df.filter((col(\"subreddit\").isNotNull()) & (col(\"summary\").isNotNull()) & (col(\"content\").isNotNull()))\n",
    "\n",
    "# Show filtered data\n",
    "df_filtered.select(\"subreddit\", \"summary\", \"content\") # .show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a8547-ff5d-425d-8787-f5a06bc7933b",
   "metadata": {},
   "source": [
    "## We have to make the Text understandable for the algorithm\n",
    "\n",
    "1. We first tokenize the the columns\n",
    "2. Remove stop words, since they do not add information to the text\n",
    "3. We convert the Text with TF-IDF to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5f5d6e9-40b1-498d-b9c2-e48ed53478c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler\n",
    "\n",
    "# Tokenize summary and content\n",
    "tokenizer = Tokenizer(inputCol=\"summary\", outputCol=\"summary_tokens\")\n",
    "tokenizer2 = Tokenizer(inputCol=\"content\", outputCol=\"content_tokens\")\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"summary_tokens\", outputCol=\"summary_clean\")\n",
    "stopwords_remover2 = StopWordsRemover(inputCol=\"content_tokens\", outputCol=\"content_clean\")\n",
    "\n",
    "# Convert words to numerical features using TF-IDF\n",
    "hashing_tf = HashingTF(inputCol=\"summary_clean\", outputCol=\"summary_tf\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"summary_tf\", outputCol=\"summary_features\")\n",
    "\n",
    "hashing_tf2 = HashingTF(inputCol=\"content_clean\", outputCol=\"content_tf\", numFeatures=1000)\n",
    "idf2 = IDF(inputCol=\"content_tf\", outputCol=\"content_features\")\n",
    "\n",
    "# Convert subreddit (text label) into a numerical label\n",
    "label_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\")\n",
    "\n",
    "# Combine summary and content features\n",
    "feature_assembler = VectorAssembler(inputCols=[\"summary_features\", \"content_features\"], outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda02fd-4a56-4c4a-8fd4-05e552b85694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate model accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0b65ad2-c53e-438c-878e-e5b2b416302c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/09 21:51:11 ERROR TaskSchedulerImpl: Lost executor 1 on 192.168.2.134: Command exited with code 52\n",
      "25/03/09 21:51:11 ERROR TaskSchedulerImpl: Lost executor 3 on 192.168.2.178: Command exited with code 52\n",
      "25/03/09 21:51:12 ERROR TaskSchedulerImpl: Lost executor 0 on 192.168.2.144: Command exited with code 52\n",
      "25/03/09 21:51:13 ERROR TaskSchedulerImpl: Lost executor 2 on 192.168.2.131: Command exited with code 52\n",
      "25/03/09 21:51:13 ERROR TaskSchedulerImpl: Lost executor 5 on 192.168.2.144: Command exited with code 52\n",
      "25/03/09 21:51:18 ERROR TaskSchedulerImpl: Lost executor 6 on 192.168.2.237: Command exited with code 52\n",
      "25/03/09 21:51:32 ERROR TaskSchedulerImpl: Lost executor 7 on 192.168.2.57: Command exited with code 52\n",
      "25/03/09 21:51:32 ERROR TaskSchedulerImpl: Lost executor 8 on 192.168.2.134: Command exited with code 52\n",
      "25/03/09 21:51:32 ERROR TaskSchedulerImpl: Lost executor 9 on 192.168.2.131: Command exited with code 52\n",
      "25/03/09 21:51:32 ERROR TaskSchedulerImpl: Lost executor 4 on 192.168.2.11: Command exited with code 52\n",
      "25/03/09 21:51:33 ERROR TaskSchedulerImpl: Lost executor 11 on 192.168.2.178: Command exited with code 52\n",
      "25/03/09 21:51:34 ERROR TaskSchedulerImpl: Lost executor 10 on 192.168.2.144: Command exited with code 52\n",
      "25/03/09 21:51:34 ERROR TaskSchedulerImpl: Lost executor 12 on 192.168.2.144: Command exited with code 52\n",
      "25/03/09 21:51:34 ERROR TaskSetManager: Task 9 in stage 28.0 failed 4 times; aborting job\n",
      "25/03/09 21:51:34 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 28.0 failed 4 times, most recent failure: Lost task 9.4 in stage 28.0 (TID 427) (192.168.2.144 executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Command exited with code 52\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o116.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 28.0 failed 4 times, most recent failure: Lost task 9.4 in stage 28.0 (TID 427) (192.168.2.144 executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Command exited with code 52\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m pipeline_rf \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[tokenizer, tokenizer2, stopwords_remover, stopwords_remover2,\n\u001b[1;32m     11\u001b[0m                                hashing_tf, idf, hashing_tf2, idf2, label_indexer, feature_assembler, rf_classifier])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model_rf \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m model_rf\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://192.168.2.156:9000/data/reddit/model/reddit_text_classifier_rf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o116.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 28.0 failed 4 times, most recent failure: Lost task 9.4 in stage 28.0 (TID 427) (192.168.2.144 executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Command exited with code 52\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "train_data, test_data = df_filtered.sample(fraction=0.02, seed=42).randomSplit([0.8, 0.2])\n",
    "\n",
    "# Define the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
    "\n",
    "# Create a new pipeline using Random Forest instead of Logistic Regression\n",
    "pipeline_rf = Pipeline(stages=[tokenizer, tokenizer2, stopwords_remover, stopwords_remover2,\n",
    "                               hashing_tf, idf, hashing_tf2, idf2, label_indexer, feature_assembler, rf_classifier])\n",
    "\n",
    "# Train the model\n",
    "model_rf = pipeline_rf.fit(train_data)\n",
    "\n",
    "# Save the trained model\n",
    "model_rf.save(\"hdfs://192.168.2.156:9000/data/reddit/model/reddit_text_classifier_rf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d768be0-7041-48a0-9126-10b7857b53ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/09 21:39:21 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/03/09 21:39:21 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n",
      "\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n",
      "\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:52)\n",
      "\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:31)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.phi$1(StrongWolfe.scala:76)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.$anonfun$minimizeWithBound$7(StrongWolfe.scala:152)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.minimizeWithBound(StrongWolfe.scala:151)\n",
      "\tat breeze.optimize.StrongWolfeLineSearch.minimize(StrongWolfe.scala:62)\n",
      "\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:82)\n",
      "\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:38)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.$anonfun$infiniteIterations$1(FirstOrderMinimizer.scala:63)\n",
      "\tat scala.collection.Iterator$$anon$7.next(Iterator.scala:140)\n",
      "\tat breeze.util.IteratorImplicits$RichIterator$$anon$2.next(Implicits.scala:79)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1015)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:634)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/03/09 21:39:21 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@55a6d3f9[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@7179b586[Wrapped task = org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint$$anon$2@604f975d]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@23f560ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:340)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:562)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.$anonfun$onDisconnected$1(StandaloneSchedulerBackend.scala:346)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.onDisconnected(StandaloneSchedulerBackend.scala:313)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/09 21:39:21 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@38a2582d[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@68556135[Wrapped task = org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint$$anon$2@16137810]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@23f560ee[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:340)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:562)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.$anonfun$onDisconnected$1(StandaloneSchedulerBackend.scala:346)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint.onDisconnected(StandaloneSchedulerBackend.scala:313)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/09 21:39:21 ERROR Utils: Uncaught exception in thread stop-spark-context\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient.stop(StandaloneAppClient.scala:288)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:275)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:142)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop$(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stop(CoarseGrainedSchedulerBackend.scala:54)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$stop$2(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$4(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2198)\n",
      "Caused by: org.apache.spark.SparkException: Could not find AppClient.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:72)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o383.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:52)\n\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:31)\n\tat breeze.optimize.StrongWolfeLineSearch.phi$1(StrongWolfe.scala:76)\n\tat breeze.optimize.StrongWolfeLineSearch.$anonfun$minimizeWithBound$7(StrongWolfe.scala:152)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n\tat breeze.optimize.StrongWolfeLineSearch.minimizeWithBound(StrongWolfe.scala:151)\n\tat breeze.optimize.StrongWolfeLineSearch.minimize(StrongWolfe.scala:62)\n\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:82)\n\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:38)\n\tat breeze.optimize.FirstOrderMinimizer.$anonfun$infiniteIterations$1(FirstOrderMinimizer.scala:63)\n\tat scala.collection.Iterator$$anon$7.next(Iterator.scala:140)\n\tat breeze.util.IteratorImplicits$RichIterator$$anon$2.next(Implicits.scala:79)\n\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1015)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:634)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[tokenizer, tokenizer2, stopwords_remover, stopwords_remover2,\n\u001b[1;32m     14\u001b[0m                             hashing_tf, idf, hashing_tf2, idf2, label_indexer, feature_assembler, classifier])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# model.save(\"hdfs://192.168.2.156:9000/data/reddit/model/reddit_text_classifier\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o383.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:52)\n\tat breeze.optimize.LineSearch$$anon$1.calculate(LineSearch.scala:31)\n\tat breeze.optimize.StrongWolfeLineSearch.phi$1(StrongWolfe.scala:76)\n\tat breeze.optimize.StrongWolfeLineSearch.$anonfun$minimizeWithBound$7(StrongWolfe.scala:152)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n\tat breeze.optimize.StrongWolfeLineSearch.minimizeWithBound(StrongWolfe.scala:151)\n\tat breeze.optimize.StrongWolfeLineSearch.minimize(StrongWolfe.scala:62)\n\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:82)\n\tat breeze.optimize.LBFGS.determineStepSize(LBFGS.scala:38)\n\tat breeze.optimize.FirstOrderMinimizer.$anonfun$infiniteIterations$1(FirstOrderMinimizer.scala:63)\n\tat scala.collection.Iterator$$anon$7.next(Iterator.scala:140)\n\tat breeze.util.IteratorImplicits$RichIterator$$anon$2.next(Implicits.scala:79)\n\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1015)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:634)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Split data into training and test sets\n",
    "# train_data, test_data = df_filtered.randomSplit([0.8, 0.2], seed=42)\n",
    "train_data, test_data = df_filtered.sample(fraction=0.02, seed=42).randomSplit([0.8, 0.2])\n",
    "\n",
    "\n",
    "# Define the classification model\n",
    "classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# Create a pipeline to apply all transformations and train the model\n",
    "pipeline = Pipeline(stages=[tokenizer, tokenizer2, stopwords_remover, stopwords_remover2,\n",
    "                            hashing_tf, idf, hashing_tf2, idf2, label_indexer, feature_assembler, classifier])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Save the trained model\n",
    "# model.save(\"hdfs://192.168.2.156:9000/data/reddit/model/reddit_text_classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5dcb2-ca24-4745-9110-884f1f5f5c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169bdc0-8af5-4948-9c18-1f438006432c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092dd2b-e265-41fe-a832-38c8d38d8f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c12129-08fd-48d2-bdac-983d5af5c9b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# release the cores for another application!\n",
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652336a3-1a25-4616-81ce-ac2cd1e1874b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7ba16-375f-4236-bbee-1776bc00f0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
