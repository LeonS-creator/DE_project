{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb861e81-8564-4107-82cf-b14a07c4834a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Setting up Spark Session / Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d52b3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 2.2.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_95814/3055322201.py\", line 9, in <module>\n",
      "    from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import (\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/ml/base.py\", line 40, in <module>\n",
      "    from pyspark.ml.param import P\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/ml/param/__init__.py\", line 35, in <module>\n",
      "    from pyspark.ml.linalg import DenseVector, Vector, Matrix\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/ml/linalg/__init__.py\", line 81, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/__init__.py\", line 267, in <module>\n",
      "    from ._csr import *\n",
      "  File \"/usr/lib/python3/dist-packages/scipy/sparse/_csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, BooleanType, DoubleType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756be08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark cluster init\n",
    "def bytes_to_mb(size_bytes):\n",
    "    \"\"\"Converts bytes to megabytes.\"\"\"\n",
    "    return size_bytes / (1024 * 1024)\n",
    "\n",
    "def bytes_to_gb(size_bytes):\n",
    "    \"\"\"Converts bytes to gigabytes.\"\"\"\n",
    "    return size_bytes / (1024 * 1024 * 1024)\n",
    "\n",
    "def configure_spark(dataset_size_gb):\n",
    "    \"\"\"Configures Spark based on dataset size.\n",
    "\n",
    "    Args:\n",
    "        dataset_size_gb (float): Size of the dataset in gigabytes.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Executor cores and memory configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    core_factor   = 2\n",
    "    memory_factor = 1\n",
    "    executor_cores  = int(dataset_size_gb * core_factor) + 2\n",
    "    # Ensure executor_cores is even\n",
    "    if executor_cores % 2 != 0:\n",
    "        executor_cores += 1\n",
    "    executor_memory = f\"{min(executor_cores, 4) * memory_factor}g\"\n",
    "    \n",
    "    return executor_cores, executor_memory\n",
    "\n",
    "\n",
    "def build_spark_session(hdfs_path, file_path, verbose=False):\n",
    "    \"\"\"Builds a Spark session and retrieves file size from HDFS.\n",
    "\n",
    "    Args:\n",
    "        hdfs_path (str): HDFS path.\n",
    "        file_path (str): File path within HDFS.\n",
    "        verbose (bool, optional): Enable verbose output. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: SparkSession, SparkContext, and file size.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.appName(\"Project Group 32 HDFSFileSize\").getOrCreate()\n",
    "    jvm = spark._jvm\n",
    "    conf = jvm.org.apache.hadoop.conf.Configuration()\n",
    "    fs = jvm.org.apache.hadoop.fs.FileSystem.get(jvm.java.net.URI.create(hdfs_path), conf)\n",
    "    path = jvm.org.apache.hadoop.fs.Path(file_path)\n",
    "    fileStatus = fs.getFileStatus(path)\n",
    "    fileSize = fileStatus.getLen()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"File size in bytes: {fileSize}\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "    executor_cores, executor_memory = configure_spark(bytes_to_gb(fileSize))\n",
    "\n",
    "    spark_session = SparkSession.builder\\\n",
    "            .master(\"spark://192.168.2.156:7077\") \\\n",
    "            .appName(\"Project Group 32\")\\\n",
    "            .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "            .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "            .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "            .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"60s\")\\\n",
    "            .config(\"spark.executor.cores\", min(executor_cores, 4))\\\n",
    "            .config(\"spark.executor.memory\", executor_memory)\\\n",
    "            .config(\"spark.cores.max\", min(executor_cores, 32))\\\n",
    "            .config(\"spark.driver.port\",9999)\\\n",
    "            .config(\"spark.blockManager.port\",10005)\\\n",
    "            .getOrCreate()\n",
    "\n",
    "    # RDD API\n",
    "    spark_context = spark_session.sparkContext\n",
    "    spark_context.setLogLevel(\"ERROR\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"A files size of {bytes_to_gb(fileSize):.4f} GB give a maximum \\n\"+\n",
    "              f\"of {spark_session.conf.get('spark.cores.max')} cores divided on spark executors with:\\n\"+\n",
    "            f\"Executor cores: {spark_session.conf.get('spark.executor.cores')}\\n\"+\n",
    "            f\"Executor memory: {spark_session.conf.get('spark.executor.memory')}\\n\"+\n",
    "            f\"Mem/core: {int(spark_session.conf.get('spark.executor.memory')[:-1])/int(spark_session.conf.get('spark.executor.cores')):.0f}GB\")\n",
    "\n",
    "    return spark_session, spark_context, fileSize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c388e54-f3ff-418e-865e-618c7194132f",
   "metadata": {},
   "source": [
    "## Create a dataframe to analyse the posts line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db317b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(spark_session, hdfs_path, file_path, fileSize, schema=None, verbose=False):\n",
    "    \"\"\"Loads JSON data from HDFS into a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spark_session (SparkSession): Spark session.\n",
    "        hdfs_path (str): HDFS path.\n",
    "        file_path (str): File path within HDFS.\n",
    "        fileSize (int): Size of the file in bytes.\n",
    "        schema: Schema for the JSON object. Defaults to None which infers schema from the data.\n",
    "        verbose (bool, optional): Enable verbose output. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Loaded Spark DataFrame.\n",
    "    \"\"\"\n",
    "    # Load JSON file into a Spark DataFrame\n",
    "    if schema is None:\n",
    "        df = spark_session.read.json(hdfs_path + file_path)\n",
    "        # Show schema to understand the structure\n",
    "        print(\"The schema:\")\n",
    "        df.printSchema()\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        df = spark_session.read.json(hdfs_path + file_path, schema=schema)\n",
    "\n",
    "    if verbose:\n",
    "        # Count the number of partitions in the underlying RDD.\n",
    "        print(f\"Number of default partitions after loading the data: {df.rdd.getNumPartitions()}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Repartition using \"subreddit\" as key.\n",
    "    # The partition size matches the HDFS block size in MB.\n",
    "    no_partitions = math.ceil(bytes_to_mb(fileSize) / 128)\n",
    "    partition_key =  \"subreddit\"\n",
    "    df.repartition(no_partitions, partition_key)\n",
    "    if verbose:\n",
    "        print(f\"The data is now repartitoned on key: '{partition_key}', into {df.rdd.getNumPartitions()} partitions.\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682b100-d96f-4f0e-abe7-90d2a8bebb5c",
   "metadata": {},
   "source": [
    "## How many Subreddits do exist?\n",
    "\n",
    "-- We see that many post are not assigned to a Subreddit, since we want to train a Classification model, we delete the NULL post --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae10602-3066-4837-b87e-dbf81fae1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df):\n",
    "\n",
    "    # Filter out NULL subreddit, summary, or content\n",
    "    df_filtered = df.filter((col(\"subreddit\").isNotNull()) & (col(\"summary\").isNotNull()) & (col(\"content\").isNotNull()))\n",
    "\n",
    "    # Group on subreddit and create a \"count\" for each in descending order\n",
    "    df_counts = df_filtered.groupBy(\"subreddit\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "    # Retrieve the top 25 subreddits\n",
    "    top_25_counts = df_counts.limit(25)\n",
    "    \n",
    "    # Collect (transfer them locally, not distributed) and put into a list\n",
    "    top_25_subreddits = [row.subreddit for row in top_25_counts.collect()]\n",
    "\n",
    "    # Filter the null-filtered data based on the top 25 subreddits\n",
    "    df_filtered = df_filtered.filter(col(\"subreddit\").isin(top_25_subreddits))\n",
    "\n",
    "    return df_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1cab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, seed=42, test_fraction=0.2):\n",
    "    # Split data into training and test sets\n",
    "    train_data, test_data = df.randomSplit([(1-test_fraction), test_fraction], seed=seed)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595011a3-7885-4dea-ad15-a6224a8abf27",
   "metadata": {},
   "source": [
    "## To prepare the Data for our ML Classification Model, we use the columns summary and content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7ac53",
   "metadata": {},
   "source": [
    "## Create the training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a8547-ff5d-425d-8787-f5a06bc7933b",
   "metadata": {},
   "source": [
    "## We have to make the Text understandable for the algorithm\n",
    "\n",
    "1. We first tokenize the the columns\n",
    "2. Remove stop words, since they do not add information to the text\n",
    "3. We convert the Text with TF-IDF to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f5d6e9-40b1-498d-b9c2-e48ed53478c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_pipe():\n",
    "    \"\"\"\n",
    "    Creates a pipeline for pre-processing text data for machine learning.\n",
    "\n",
    "    This pipeline includes tokenization, stop word removal, TF-IDF vectorization,\n",
    "    label indexing, and feature assembly.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Spark ML pipeline stages.\n",
    "    \"\"\"\n",
    "    # Tokenize summary and content\n",
    "    tokenizer  = Tokenizer(inputCol=\"summary\", outputCol=\"summary_tokens\")\n",
    "    tokenizer2 = Tokenizer(inputCol=\"content\", outputCol=\"content_tokens\")\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords_remover  = StopWordsRemover(inputCol=\"summary_tokens\", outputCol=\"summary_clean\")\n",
    "    stopwords_remover2 = StopWordsRemover(inputCol=\"content_tokens\", outputCol=\"content_clean\")\n",
    "\n",
    "    # Convert words to numerical features using TF-IDF\n",
    "    hashing_tf = HashingTF(inputCol=\"summary_clean\", outputCol=\"summary_tf\", numFeatures=1000)\n",
    "    idf = IDF(inputCol=\"summary_tf\", outputCol=\"summary_features\")\n",
    "\n",
    "    hashing_tf2 = HashingTF(inputCol=\"content_clean\", outputCol=\"content_tf\", numFeatures=1000)\n",
    "    idf2 = IDF(inputCol=\"content_tf\", outputCol=\"content_features\")\n",
    "\n",
    "    # Convert subreddit (text label) into a numerical label\n",
    "    label_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "\n",
    "    # Combine summary and content features\n",
    "    feature_assembler = VectorAssembler(inputCols=[\"summary_features\", \"content_features\"], outputCol=\"features\")\n",
    "\n",
    "    # Return pre-processing pipeline.\n",
    "    return [tokenizer, tokenizer2, stopwords_remover, stopwords_remover2,\n",
    "            hashing_tf, idf, hashing_tf2, idf2, label_indexer, feature_assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4532bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_eval(model, test_data, description=\"\", verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluates a machine learning model's accuracy on test data.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Spark ML model.\n",
    "        test_data (DataFrame): The test dataset.\n",
    "        description (str, optional): A description of the model for output. Defaults to \"\".\n",
    "        verbose (bool, optional): Enable verbose output. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model.\n",
    "    \"\"\"\n",
    "    # Make predictions on test data\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Evaluate model accuracy\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "    if verbose:\n",
    "      print(f\"Evaluation of {description}. \\n\"+\n",
    "            f\"Model Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0b65ad2-c53e-438c-878e-e5b2b416302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def random_forest(train_data, pre_pipe):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest classification model.\n",
    "\n",
    "    Args:\n",
    "        train_data (DataFrame): The training dataset.\n",
    "        pre_pipe (list): List of pre-processing stages.\n",
    "\n",
    "    Returns:\n",
    "        PipelineModel: The trained Random Forest model.\n",
    "    \"\"\"\n",
    "    # Define the Random Forest classifier\n",
    "    classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
    "\n",
    "    # Create a new pipeline using Random Forest\n",
    "    pipeline = Pipeline(stages= pre_pipe + [classifier])\n",
    "\n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    # Save the trained model\n",
    "    #model.save(\"hdfs://192.168.2.156:9000/data/reddit/model/reddit_text_classifier_rf\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d768be0-7041-48a0-9126-10b7857b53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logistic_regression(train_data, pre_pipe):\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression classification model.\n",
    "\n",
    "    Args:\n",
    "        train_data (DataFrame): The training dataset.\n",
    "        pre_pipe (list): List of pre-processing stages.\n",
    "\n",
    "    Returns:\n",
    "        PipelineModel: The trained Logistic Regression model.\n",
    "    \"\"\"\n",
    "    # Define the classification model\n",
    "    classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "    # Create a new pipeline using Logistic Regression\n",
    "    pipeline = Pipeline(stages= pre_pipe + [classifier])\n",
    "\n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    # Save the trained model\n",
    "    # model.save(\"hdfs://192.168.2.156:9000/data/reddit/model/reddit_text_classifier\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebfc4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(i, fileSize, executor_cores, executor_memory, max_cores, data_load_time, training_time, evaluation_time, overall_exec_time, model_accuracy):\n",
    "    \"\"\"\n",
    "    Prints performance and evaluation results.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path of the input file.\n",
    "        fileSize (float): Size of the input file in GB.\n",
    "        no_samples (int): Number of samples processed.\n",
    "        executor_cores (int): Number of executor cores.\n",
    "        executor_memory (str): Executor memory configuration.\n",
    "        execution_time (float): Total execution time in seconds.\n",
    "        accuracy_rf (float): Accuracy of the Random Forest model.\n",
    "        accuracy_lr (float): Accuracy of the Logistic Regression model.\n",
    "    \"\"\"\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Spark Processing and Model Evaluation Results\\n\")\n",
    "    print(f\"Iteration {i}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"File Size:        {fileSize:.2f} GB\")\n",
    "    print(f\"Max cores:        {max_cores}\")\n",
    "    print(f\"Executor Cores:   {executor_cores}\")\n",
    "    print(f\"Executor Memory:  {executor_memory}\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Performance Metrics:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"Data load Time:         {data_load_time:.2f} seconds\")\n",
    "    print(f\"Training Time:          {training_time:.2f} seconds\")\n",
    "    print(f\"Evaluation Time:        {evaluation_time:.2f} seconds\")\n",
    "    print(f\"Overall execution Time: {overall_exec_time:.2f} seconds\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Model Accuracy:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"Random Forest Accuracy:     {model_accuracy:.4f}\")\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769f1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(hdfs_path, file_path, schema, n=5, verbose=False):\n",
    "\n",
    "    overall_exec_time   = np.zeros(n)\n",
    "    data_load_time      = np.zeros(n)\n",
    "    training_time       = np.zeros(n)\n",
    "    evaluation_time     = np.zeros(n)\n",
    "    model_accuracy      = np.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        print(f\"File {file_path}, run {i}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create a spark session\n",
    "        spark_session, spark_context, fileSize = build_spark_session(hdfs_path, file_path, verbose=verbose)\n",
    "        \n",
    "        # Load the data\n",
    "        df = load_data(spark_session, hdfs_path, file_path, fileSize, schema=schema, verbose=verbose)\n",
    "\n",
    "        # Filter the data\n",
    "        df = filter_data(df)\n",
    "\n",
    "        # Split data into training and test sets\n",
    "        train_data, test_data = split_data(df)\n",
    "\n",
    "        # Save time for data load/transform\n",
    "        data_time = time.time()\n",
    "        data_load_time[i] = data_time - start_time\n",
    "\n",
    "        # Create a pipeline for the pre-processing\n",
    "        pre_pipe = pre_processing_pipe()\n",
    "        # Create and train a ML model for classification\n",
    "        model_rf = random_forest(train_data, pre_pipe)\n",
    "\n",
    "        # Save time for model training\n",
    "        train_time = time.time()\n",
    "        training_time[i] = train_time - data_time\n",
    "\n",
    "        # Evaluate the performance of the ML model on the test data\n",
    "        model_accuracy[i] = model_eval(model_rf, test_data, description=\"Random forest classifier\", verbose=verbose)\n",
    "\n",
    "        # Save time for model evaluation\n",
    "        eval_time = time.time()\n",
    "        evaluation_time[i] = eval_time - train_time\n",
    "\n",
    "        executor_cores = spark_session.conf.get(\"spark.executor.cores\")\n",
    "        executor_memory = spark_session.conf.get(\"spark.executor.memory\")\n",
    "        max_cores = spark_session.conf.get('spark.cores.max')\n",
    "\n",
    "        spark_context.stop()\n",
    "\n",
    "        # Determine overall execution time\n",
    "        end_time = time.time()\n",
    "        overall_exec_time[i] = end_time - start_time\n",
    "        \n",
    "        if verbose:\n",
    "            print_results(i, bytes_to_gb(fileSize), executor_cores, executor_memory, max_cores, data_load_time[i], training_time[i], evaluation_time[i], overall_exec_time[i], model_accuracy[i])\n",
    "\n",
    "    return [bytes_to_gb(fileSize), executor_cores, executor_memory, max_cores, data_load_time.mean(), training_time.mean(), evaluation_time.mean(), overall_exec_time.mean(), model_accuracy.mean()]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38f32162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schemas for the reddit data\n",
    "subreddit_field = StructField(name=\"subreddit\", dataType=StringType(), nullable=True)\n",
    "summary_field   = StructField(name=\"summary\",   dataType=StringType(), nullable=True)\n",
    "content_field   = StructField(name=\"content\",   dataType=StringType(), nullable=True)\n",
    "body_field      = StructField(name=\"body\",      dataType=StringType(), nullable=True)\n",
    "\n",
    "schema_v0 = StructType([subreddit_field])\n",
    "schema_v1 = StructType([subreddit_field, summary_field, content_field])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a48d2e",
   "metadata": {},
   "source": [
    "Evaluate all reddit datasets five times and save the average as a result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd171298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_50k.json, run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/11 22:03:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /data/reddit/reddit_50k.json, run 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "hdfs_path = \"hdfs://192.168.2.156:9000\"\n",
    "\n",
    "file_path = \"/data/reddit/\"\n",
    "\n",
    "files = [\"reddit_50k.json\", \"reddit_100k.json\", \n",
    "         \"reddit_200k.json\", \"reddit_500k.json\", \n",
    "         \"corpus-webis-tldr-17.json\"]\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        res = evaluate_performance(hdfs_path, f\"{file_path}{file}\", schema=schema_v1, n=5)\n",
    "        results.append([file[:-5]] + res)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Crashed when evaluating {file} with error:\")\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(results, columns=['File', 'File size', 'Executor cores', 'Executor memory', 'Max cores', 'Data load time', 'Training time', 'Evaluation time', 'Overall exec time', 'Model accuracy']) \n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eaa008",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/home/ubuntu/out/\"\n",
    "result_df.to_csv(path_or_buf=f\"{out_path}performance_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a462cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl \n",
    "with pd.ExcelWriter(f\"{out_path}performance_data.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    result_df.to_excel(writer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652336a3-1a25-4616-81ce-ac2cd1e1874b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7ba16-375f-4236-bbee-1776bc00f0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
