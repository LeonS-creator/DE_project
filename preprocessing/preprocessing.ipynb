{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb861e81-8564-4107-82cf-b14a07c4834a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Setting up Spark Session / Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4d52b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "377e74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_mb(size_bytes):\n",
    "    return size_bytes / (1024 * 1024)\n",
    "\n",
    "def bytes_to_gb(size_bytes):\n",
    "    return size_bytes / (1024 * 1024 * 1024)\n",
    "\n",
    "def configure_spark(dataset_size_gb):\n",
    "    \"\"\"Configures Spark based on dataset size.\"\"\"\n",
    "    if dataset_size_gb < 1:\n",
    "        executor_cores = 2\n",
    "        executor_memory = \"4g\"\n",
    "    elif 1 <= dataset_size_gb <= 10:\n",
    "        executor_cores = 4\n",
    "        executor_memory = \"8g\"\n",
    "    else:\n",
    "        executor_cores = 8\n",
    "        executor_memory = \"16g\"\n",
    "    return executor_cores, executor_memory\n",
    "\n",
    "\n",
    "def build_spark_session(hdfs_path, file_path, verbose=False):\n",
    "    spark = SparkSession.builder.appName(\"Project Group 32 HDFSFileSize\").getOrCreate()\n",
    "    jvm = spark._jvm\n",
    "    conf = jvm.org.apache.hadoop.conf.Configuration()\n",
    "    fs = jvm.org.apache.hadoop.fs.FileSystem.get(jvm.java.net.URI.create(hdfs_path), conf)\n",
    "    path = jvm.org.apache.hadoop.fs.Path(file_path)\n",
    "    fileStatus = fs.getFileStatus(path)\n",
    "    fileSize = fileStatus.getLen()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"File size in bytes: {fileSize}\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "    executor_cores, executor_memory = configure_spark(bytes_to_gb(fileSize))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"A files size of {bytes_to_gb(fileSize):.4f} GB give spark executors with:\\n\"+\n",
    "            f\"Cores: {executor_cores}\\n\"+\n",
    "            f\"Mem/core: {int(executor_memory[:-1])/executor_cores:.0f}GB\")\n",
    "\n",
    "\n",
    "    spark_session = SparkSession.builder\\\n",
    "            .master(\"spark://192.168.2.156:7077\") \\\n",
    "            .appName(\"Project Group 32 Andreas\")\\\n",
    "            .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "            .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "            .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "            .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"60s\")\\\n",
    "            .config(\"spark.executor.cores\", executor_cores)\\\n",
    "            .config(\"spark.executor.memory\", executor_memory)\\\n",
    "            .config(\"spark.driver.port\",9999)\\\n",
    "            .config(\"spark.blockManager.port\",10005)\\\n",
    "            .getOrCreate()\n",
    "\n",
    "    # RDD API\n",
    "    spark_context = spark_session.sparkContext\n",
    "    spark_context.setLogLevel(\"ERROR\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Executor cores: {spark_session.conf.get('spark.executor.cores')}\")\n",
    "        print(f\"Executor memory: {spark_session.conf.get('spark.executor.memory')}\")\n",
    "\n",
    "    return spark_session, spark_context, fileSize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c388e54-f3ff-418e-865e-618c7194132f",
   "metadata": {},
   "source": [
    "## Create a dataframe to analyse the posts line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "db317b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(spark_session, hdfs_path, file_path, fileSize, verbose=False):\n",
    "    # Load JSON file into a Spark DataFrame\n",
    "    df = spark_session.read.json(hdfs_path + file_path)\n",
    "\n",
    "    if verbose:\n",
    "        # Count the number of partitions in the underlying RDD.\n",
    "        print(f\"Number of default partitions after loading the data: {df.rdd.getNumPartitions()}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Repartition using \"subreddit\" as key.\n",
    "    # The partition size matches the HDFS block size in MB.\n",
    "    no_partitions = math.ceil(bytes_to_mb(fileSize) / 128)\n",
    "    partition_key =  \"subreddit\"\n",
    "    df.repartition(no_partitions, partition_key)\n",
    "    if verbose:\n",
    "        print(f\"The data is now repartitoned on key: '{partition_key}', into {no_partitions} partitions.\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Count the number of partitions after repartitioning.\n",
    "        print(f\"Number of default partitions after repartitioning: {df.rdd.getNumPartitions()}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Show schema to understand the structure\n",
    "        print(\"The schema:\")\n",
    "        df.printSchema()\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Show first few rows to inspect data\n",
    "        print(\"The first five entries in the dataframe:\")\n",
    "        df.show(5, truncate=False)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Count total number of rows\n",
    "        print(f\"Total Rows: {df.count()}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682b100-d96f-4f0e-abe7-90d2a8bebb5c",
   "metadata": {},
   "source": [
    "## How many Subreddits do exist?\n",
    "\n",
    "-- We see that many post are not assigned to a Subreddit, since we want to train a Classification model, we delete the NULL post --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3ae10602-3066-4837-b87e-dbf81fae1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_and_split_data(df, seed=42, verbose=False):\n",
    "    unique_subreddits = df.select(\"subreddit\").distinct().count()\n",
    "    if verbose:\n",
    "        print(f\"Unique Subreddits: {unique_subreddits}\")\n",
    "        df.groupBy(\"subreddit\").count().orderBy(col(\"count\").desc()).show(10, False)\n",
    "    else:\n",
    "        df.groupBy(\"subreddit\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "    # Filter out rows where subreddit is NULL\n",
    "    df_filtered = df.filter(col(\"subreddit\").isNotNull())\n",
    "\n",
    "    if verbose:\n",
    "        # Show first few rows after filtering\n",
    "        df_filtered.show(5, truncate=False)\n",
    "\n",
    "        # Count remaining rows\n",
    "        print(f\"Total Posts After Filtering: {df_filtered.count()}\")\n",
    "\n",
    "    # Filter out NULL subreddit, summary, or content\n",
    "    df_filtered = df.filter((col(\"subreddit\").isNotNull()) & (col(\"summary\").isNotNull()) & (col(\"content\").isNotNull()))\n",
    "\n",
    "    if verbose:\n",
    "        # Show filtered data\n",
    "        df_filtered.select(\"subreddit\", \"summary\", \"content\").show(5, truncate=False)\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    train_data, test_data = df_filtered.randomSplit([0.8, 0.2], seed=seed)\n",
    "\n",
    "    return train_data, test_data, df_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595011a3-7885-4dea-ad15-a6224a8abf27",
   "metadata": {},
   "source": [
    "## To prepare the Data for our ML Classification Model, we use the columns summary and content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7ac53",
   "metadata": {},
   "source": [
    "## Create the training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a8547-ff5d-425d-8787-f5a06bc7933b",
   "metadata": {},
   "source": [
    "## We have to make the Text understandable for the algorithm\n",
    "\n",
    "1. We first tokenize the the columns\n",
    "2. Remove stop words, since they do not add information to the text\n",
    "3. We convert the Text with TF-IDF to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a5f5d6e9-40b1-498d-b9c2-e48ed53478c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_pipe():\n",
    "    # Tokenize summary and content\n",
    "    tokenizer  = Tokenizer(inputCol=\"summary\", outputCol=\"summary_tokens\")\n",
    "    tokenizer2 = Tokenizer(inputCol=\"content\", outputCol=\"content_tokens\")\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords_remover  = StopWordsRemover(inputCol=\"summary_tokens\", outputCol=\"summary_clean\")\n",
    "    stopwords_remover2 = StopWordsRemover(inputCol=\"content_tokens\", outputCol=\"content_clean\")\n",
    "\n",
    "    # Convert words to numerical features using TF-IDF\n",
    "    hashing_tf = HashingTF(inputCol=\"summary_clean\", outputCol=\"summary_tf\", numFeatures=1000)\n",
    "    idf = IDF(inputCol=\"summary_tf\", outputCol=\"summary_features\")\n",
    "\n",
    "    hashing_tf2 = HashingTF(inputCol=\"content_clean\", outputCol=\"content_tf\", numFeatures=1000)\n",
    "    idf2 = IDF(inputCol=\"content_tf\", outputCol=\"content_features\")\n",
    "\n",
    "    # Convert subreddit (text label) into a numerical label\n",
    "    label_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "\n",
    "    # Combine summary and content features\n",
    "    feature_assembler = VectorAssembler(inputCols=[\"summary_features\", \"content_features\"], outputCol=\"features\")\n",
    "\n",
    "    # Return pre-processing pipeline.\n",
    "    return [tokenizer, tokenizer2, stopwords_remover, stopwords_remover2,\n",
    "            hashing_tf, idf, hashing_tf2, idf2, label_indexer, feature_assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4532bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_eval(model, test_data, description=\"\", verbose=False):\n",
    "    # Make predictions on test data\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Evaluate model accuracy\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "    if verbose:\n",
    "      print(f\"Evaluation of {description}. \\n\"+\n",
    "            f\"Model Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d0b65ad2-c53e-438c-878e-e5b2b416302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def random_forest(train_data, pre_pipe):\n",
    "    # Define the Random Forest classifier\n",
    "    classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
    "\n",
    "    # Create a new pipeline using Random Forest\n",
    "    pipeline = Pipeline(stages= pre_pipe + [classifier])\n",
    "\n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    # Save the trained model\n",
    "    #model.save(\"hdfs://192.168.2.156:9000/data/reddit/model/reddit_text_classifier_rf\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7d768be0-7041-48a0-9126-10b7857b53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logistic_regression(train_data, pre_pipe):\n",
    "    # Define the classification model\n",
    "    classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "    # Create a new pipeline using Logistic Regression\n",
    "    pipeline = Pipeline(stages= pre_pipe + [classifier])\n",
    "\n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    # Save the trained model\n",
    "    # model.save(\"hdfs://192.168.2.156:9000/data/reddit/model/reddit_text_classifier\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ebfc4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(file_path, fileSize, no_samples, executor_cores, executor_memory, execution_time, accuracy_rf, accuracy_lr):\n",
    "    \"\"\"\n",
    "    Performance and evaluation results\n",
    "    \"\"\"\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Spark Processing and Model Evaluation Results\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"File Path:        {file_path}\")\n",
    "    print(f\"File Size:        {fileSize:.2f} GB\")\n",
    "    print(f\"No samples:       {no_samples}\")\n",
    "    print(f\"Executor Cores:   {executor_cores}\")\n",
    "    print(f\"Executor Memory:  {executor_memory}\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Performance Metrics:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"Execution Time:   {execution_time:.2f} seconds\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Model Accuracy:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"Random Forest Accuracy:     {accuracy_rf:.4f}\")\n",
    "    print(f\"Logistic Regression Accuracy: {accuracy_lr:.4f}\")\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "61f5dcb2-ca24-4745-9110-884f1f5f5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = \"hdfs://192.168.2.156:9000\"\n",
    "file_path = \"/data/reddit/reddit_50k.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169bdc0-8af5-4948-9c18-1f438006432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 175:>                                                        (0 + 4) / 4]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "spark_session, spark_context, fileSize = build_spark_session(hdfs_path, file_path)\n",
    "df = load_data(spark_session, hdfs_path, file_path, fileSize)\n",
    "train_data, test_data, no_samples = filter_and_split_data(df)\n",
    "pre_pipe = pre_processing_pipe()\n",
    "model_rf = random_forest(train_data, pre_pipe)\n",
    "model_lr = logistic_regression(train_data, pre_pipe)\n",
    "accuracy_rf = model_eval(model_rf, test_data, description=\"Random forest classifier\")\n",
    "accuracy_lr = model_eval(model_lr, test_data, description=\"Logistic regression classifier\")\n",
    "\n",
    "executor_cores = spark_session.conf.get(\"spark.executor.cores\")\n",
    "executor_memory = spark_session.conf.get(\"spark.executor.memory\")\n",
    "spark_context.stop()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print_results(file_path=file_path, fileSize=bytes_to_gb(fileSize), no_samples=no_samples, executor_cores=executor_cores, \n",
    "              executor_memory=executor_memory, execution_time=execution_time, \n",
    "              accuracy_rf=accuracy_rf, accuracy_lr=accuracy_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7092dd2b-e265-41fe-a832-38c8d38d8f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 21:55:50 ERROR Instrumentation: org.apache.spark.SparkException: Job 66 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c12129-08fd-48d2-bdac-983d5af5c9b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652336a3-1a25-4616-81ce-ac2cd1e1874b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7ba16-375f-4236-bbee-1776bc00f0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
