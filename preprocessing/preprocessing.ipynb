{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb861e81-8564-4107-82cf-b14a07c4834a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Setting up Spark Session / Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "4d52b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "377e74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_mb(size_bytes):\n",
    "    return size_bytes / (1024 * 1024)\n",
    "\n",
    "def bytes_to_gb(size_bytes):\n",
    "    return size_bytes / (1024 * 1024 * 1024)\n",
    "\n",
    "def configure_spark(dataset_size_gb):\n",
    "    \"\"\"Configures Spark based on dataset size.\"\"\"\n",
    "    if dataset_size_gb < 1:\n",
    "        executor_cores = 2\n",
    "        executor_memory = \"4g\"\n",
    "    elif 1 <= dataset_size_gb <= 10:\n",
    "        executor_cores = 4\n",
    "        executor_memory = \"8g\"\n",
    "    else:\n",
    "        executor_cores = 8\n",
    "        executor_memory = \"16g\"\n",
    "    return executor_cores, executor_memory\n",
    "\n",
    "\n",
    "def build_spark_session(hdfs_path, file_path, verbose=False):\n",
    "    spark = SparkSession.builder.appName(\"Project Group 32 HDFSFileSize\").getOrCreate()\n",
    "    jvm = spark._jvm\n",
    "    conf = jvm.org.apache.hadoop.conf.Configuration()\n",
    "    fs = jvm.org.apache.hadoop.fs.FileSystem.get(jvm.java.net.URI.create(hdfs_path), conf)\n",
    "    path = jvm.org.apache.hadoop.fs.Path(file_path)\n",
    "    fileStatus = fs.getFileStatus(path)\n",
    "    fileSize = fileStatus.getLen()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"File size in bytes: {fileSize}\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "    executor_cores, executor_memory = configure_spark(bytes_to_gb(fileSize))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"A files size of {bytes_to_gb(fileSize):.4f} GB give spark executors with:\\n\"+\n",
    "            f\"Cores: {executor_cores}\\n\"+\n",
    "            f\"Mem/core: {int(executor_memory[:-1])/executor_cores:.0f}GB\")\n",
    "\n",
    "\n",
    "    spark_session = SparkSession.builder\\\n",
    "            .master(\"spark://192.168.2.156:7077\") \\\n",
    "            .appName(\"Project Group 32 Andreas\")\\\n",
    "            .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "            .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "            .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "            .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"60s\")\\\n",
    "            .config(\"spark.executor.cores\", executor_cores)\\\n",
    "            .config(\"spark.executor.memory\", executor_memory)\\\n",
    "            .config(\"spark.driver.port\",9999)\\\n",
    "            .config(\"spark.blockManager.port\",10005)\\\n",
    "            .getOrCreate()\n",
    "\n",
    "    # RDD API\n",
    "    spark_context = spark_session.sparkContext\n",
    "    spark_context.setLogLevel(\"ERROR\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Executor cores: {spark_session.conf.get('spark.executor.cores')}\")\n",
    "        print(f\"Executor memory: {spark_session.conf.get('spark.executor.memory')}\")\n",
    "\n",
    "    return spark_session, spark_context, fileSize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c388e54-f3ff-418e-865e-618c7194132f",
   "metadata": {},
   "source": [
    "## Create a dataframe to analyse the posts line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "db317b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(spark_session, hdfs_path, file_path, fileSize, verbose=False):\n",
    "    # Load JSON file into a Spark DataFrame\n",
    "    df = spark_session.read.json(hdfs_path + file_path)\n",
    "\n",
    "    if verbose:\n",
    "        # Count the number of partitions in the underlying RDD.\n",
    "        print(f\"Number of default partitions after loading the data: {df.rdd.getNumPartitions()}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # Repartition using \"subreddit\" as key.\n",
    "    # The partition size matches the HDFS block size in MB.\n",
    "    no_partitions = math.ceil(bytes_to_mb(fileSize) / 128)\n",
    "    partition_key =  \"subreddit\"\n",
    "    df.repartition(no_partitions, partition_key)\n",
    "    if verbose:\n",
    "        print(f\"The data is now repartitoned on key: '{partition_key}', into {df.rdd.getNumPartitions()} partitions.\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Show schema to understand the structure\n",
    "        print(\"The schema:\")\n",
    "        df.printSchema()\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Show first few rows to inspect data\n",
    "        print(\"The first five entries in the dataframe:\")\n",
    "        df.show(5, truncate=False)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Count total number of rows\n",
    "        print(f\"Total Rows: {df.count()}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682b100-d96f-4f0e-abe7-90d2a8bebb5c",
   "metadata": {},
   "source": [
    "## How many Subreddits do exist?\n",
    "\n",
    "-- We see that many post are not assigned to a Subreddit, since we want to train a Classification model, we delete the NULL post --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3ae10602-3066-4837-b87e-dbf81fae1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_and_split_data(df, seed=42, verbose=False):\n",
    "    unique_subreddits = df.select(\"subreddit\").distinct().count()\n",
    "    if verbose:\n",
    "        print(f\"Unique Subreddits: {unique_subreddits}\")\n",
    "        df.groupBy(\"subreddit\").count().orderBy(col(\"count\").desc()).show(10, False)\n",
    "    else:\n",
    "        df.groupBy(\"subreddit\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "    # Filter out rows where subreddit is NULL\n",
    "    df_filtered = df.filter(col(\"subreddit\").isNotNull())\n",
    "\n",
    "    if verbose:\n",
    "        # Show first few rows after filtering\n",
    "        df_filtered.show(5, truncate=False)\n",
    "\n",
    "        # Count remaining rows\n",
    "        print(f\"Total Posts After Filtering: {df_filtered.count()}\")\n",
    "\n",
    "    # Filter out NULL subreddit, summary, or content\n",
    "    df_filtered = df.filter((col(\"subreddit\").isNotNull()) & (col(\"summary\").isNotNull()) & (col(\"content\").isNotNull()))\n",
    "\n",
    "    if verbose:\n",
    "        # Show filtered data\n",
    "        df_filtered.select(\"subreddit\", \"summary\", \"content\").show(5, truncate=False)\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    train_data, test_data = df_filtered.randomSplit([0.8, 0.2], seed=seed)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Training set contains {train_data.count()} samples\\n\" +\n",
    "              f\"Test set contains {test_data.count()} samples.\")\n",
    "\n",
    "    return train_data, test_data, df_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595011a3-7885-4dea-ad15-a6224a8abf27",
   "metadata": {},
   "source": [
    "## To prepare the Data for our ML Classification Model, we use the columns summary and content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7ac53",
   "metadata": {},
   "source": [
    "## Create the training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a8547-ff5d-425d-8787-f5a06bc7933b",
   "metadata": {},
   "source": [
    "## We have to make the Text understandable for the algorithm\n",
    "\n",
    "1. We first tokenize the the columns\n",
    "2. Remove stop words, since they do not add information to the text\n",
    "3. We convert the Text with TF-IDF to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a5f5d6e9-40b1-498d-b9c2-e48ed53478c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_pipe():\n",
    "    # Tokenize summary and content\n",
    "    tokenizer  = Tokenizer(inputCol=\"summary\", outputCol=\"summary_tokens\")\n",
    "    tokenizer2 = Tokenizer(inputCol=\"content\", outputCol=\"content_tokens\")\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords_remover  = StopWordsRemover(inputCol=\"summary_tokens\", outputCol=\"summary_clean\")\n",
    "    stopwords_remover2 = StopWordsRemover(inputCol=\"content_tokens\", outputCol=\"content_clean\")\n",
    "\n",
    "    # Convert words to numerical features using TF-IDF\n",
    "    hashing_tf = HashingTF(inputCol=\"summary_clean\", outputCol=\"summary_tf\", numFeatures=1000)\n",
    "    idf = IDF(inputCol=\"summary_tf\", outputCol=\"summary_features\")\n",
    "\n",
    "    hashing_tf2 = HashingTF(inputCol=\"content_clean\", outputCol=\"content_tf\", numFeatures=1000)\n",
    "    idf2 = IDF(inputCol=\"content_tf\", outputCol=\"content_features\")\n",
    "\n",
    "    # Convert subreddit (text label) into a numerical label\n",
    "    label_indexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "\n",
    "    # Combine summary and content features\n",
    "    feature_assembler = VectorAssembler(inputCols=[\"summary_features\", \"content_features\"], outputCol=\"features\")\n",
    "\n",
    "    # Return pre-processing pipeline.\n",
    "    return [tokenizer, tokenizer2, stopwords_remover, stopwords_remover2,\n",
    "            hashing_tf, idf, hashing_tf2, idf2, label_indexer, feature_assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4532bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_eval(model, test_data, description=\"\", verbose=False):\n",
    "    # Make predictions on test data\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Evaluate model accuracy\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "    if verbose:\n",
    "      print(f\"Evaluation of {description}. \\n\"+\n",
    "            f\"Model Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d0b65ad2-c53e-438c-878e-e5b2b416302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def random_forest(train_data, pre_pipe):\n",
    "    # Define the Random Forest classifier\n",
    "    classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
    "\n",
    "    # Create a new pipeline using Random Forest\n",
    "    pipeline = Pipeline(stages= pre_pipe + [classifier])\n",
    "\n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    # Save the trained model\n",
    "    #model.save(\"hdfs://192.168.2.156:9000/data/reddit/model/reddit_text_classifier_rf\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "7d768be0-7041-48a0-9126-10b7857b53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logistic_regression(train_data, pre_pipe):\n",
    "    # Define the classification model\n",
    "    classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "    # Create a new pipeline using Logistic Regression\n",
    "    pipeline = Pipeline(stages= pre_pipe + [classifier])\n",
    "\n",
    "    # Train the model\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    # Save the trained model\n",
    "    # model.save(\"hdfs://192.168.2.156:9000/data/reddit/model/reddit_text_classifier\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ebfc4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(file_path, fileSize, no_samples, executor_cores, executor_memory, execution_time, accuracy_rf, accuracy_lr):\n",
    "    \"\"\"\n",
    "    Performance and evaluation results\n",
    "    \"\"\"\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Spark Processing and Model Evaluation Results\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"File Path:        {file_path}\")\n",
    "    print(f\"File Size:        {fileSize:.2f} GB\")\n",
    "    print(f\"No samples:       {no_samples}\")\n",
    "    print(f\"Executor Cores:   {executor_cores}\")\n",
    "    print(f\"Executor Memory:  {executor_memory}\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Performance Metrics:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"Execution Time:   {execution_time:.2f} seconds\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Model Accuracy:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"Random Forest Accuracy:     {accuracy_rf:.4f}\")\n",
    "    print(f\"Logistic Regression Accuracy: {accuracy_lr:.4f}\")\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "61f5dcb2-ca24-4745-9110-884f1f5f5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = \"hdfs://192.168.2.156:9000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169bdc0-8af5-4948-9c18-1f438006432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 0\n"
     ]
    }
   ],
   "source": [
    "verbose=False\n",
    "file_path = \"/data/reddit/reddit_50k.json\"\n",
    "reddit_50_samples = np.zeros(5)\n",
    "reddit_50_time    = np.zeros(5)\n",
    "reddit_50_rf_acc  = np.zeros(5)\n",
    "reddit_50_lr_acc  = np.zeros(5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Run: {i}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    spark_session, spark_context, fileSize = build_spark_session(hdfs_path, file_path, verbose=verbose)\n",
    "    df = load_data(spark_session, hdfs_path, file_path, fileSize, verbose=verbose)\n",
    "    train_data, test_data, no_samples = filter_and_split_data(df, verbose=verbose)\n",
    "    pre_time = time.time()\n",
    "    pre_time = pre_time - start_time\n",
    "    print(f\"Pre-processing time: {pre_time:.4f}\")\n",
    "\n",
    "    pre_pipe = pre_processing_pipe()\n",
    "    model_rf = random_forest(train_data, pre_pipe)\n",
    "    rf_time  = time.time()\n",
    "    rf_time  = rf_time - start_time - pre_time\n",
    "    accuracy_rf = model_eval(model_rf, test_data, description=\"Random forest classifier\")\n",
    "    print(f\"Random forest time: {rf_time:.4f}\")\n",
    "\n",
    "    \"\"\" pre_pipe = pre_processing_pipe()\n",
    "    model_lr = logistic_regression(train_data, pre_pipe)\n",
    "    lr_time  = time.time()\n",
    "    lr_time  = lr_time - start_time - rf_time\n",
    "    accuracy_lr = model_eval(model_lr, test_data, description=\"Logistic regression classifier\")\n",
    "    print(f\"Logistic regression time: {lr_time:.4f}\") \"\"\"\n",
    "    accuracy_lr = 0\n",
    "\n",
    "    executor_cores = spark_session.conf.get(\"spark.executor.cores\")\n",
    "    executor_memory = spark_session.conf.get(\"spark.executor.memory\")\n",
    "\n",
    "    spark_context.stop()\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    reddit_50_samples[i] = no_samples\n",
    "    reddit_50_time[i]    = execution_time\n",
    "    reddit_50_rf_acc[i]  = accuracy_rf\n",
    "    reddit_50_lr_acc[i]  = accuracy_lr\n",
    "\n",
    "    \n",
    "    print_results(file_path=file_path, fileSize=bytes_to_gb(fileSize), no_samples=no_samples, executor_cores=executor_cores, \n",
    "                executor_memory=executor_memory, execution_time=execution_time, \n",
    "                accuracy_rf=accuracy_rf, accuracy_lr=accuracy_lr)\n",
    "            \n",
    "print(reddit_50_samples.mean())\n",
    "print(reddit_50_samples)\n",
    "\n",
    "print(reddit_50_time.mean())\n",
    "print(reddit_50_time)\n",
    "\n",
    "print(reddit_50_rf_acc.mean())\n",
    "print(reddit_50_rf_acc)\n",
    "\n",
    "print(reddit_50_lr_acc.mean())\n",
    "print(reddit_50_lr_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65ba20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing time: 29.8419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                         (0 + 6) / 6]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=61>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o12560.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o12624.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre-processing time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m pre_pipe \u001b[38;5;241m=\u001b[39m pre_processing_pipe()\n\u001b[0;32m---> 20\u001b[0m model_rf \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_forest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_pipe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m rf_time  \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     22\u001b[0m rf_time  \u001b[38;5;241m=\u001b[39m rf_time \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m-\u001b[39m pre_time\n",
      "Cell \u001b[0;32mIn[231], line 11\u001b[0m, in \u001b[0;36mrandom_forest\u001b[0;34m(train_data, pre_pipe)\u001b[0m\n\u001b[1;32m      8\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m pre_pipe \u001b[38;5;241m+\u001b[39m [classifier])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#model.save(\"hdfs://192.168.2.156:9000/data/reddit/model/reddit_text_classifier_rf\")\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o12624.fit"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 213:>                                                        (0 + 6) / 6]\r"
     ]
    }
   ],
   "source": [
    "verbose=False\n",
    "file_path = \"/data/reddit/reddit_100k.json\"\n",
    "reddit_100_samples = np.zeros(5)\n",
    "reddit_100_time    = np.zeros(5)\n",
    "reddit_100_rf_acc  = np.zeros(5)\n",
    "reddit_100_lr_acc  = np.zeros(5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Run: {i}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    spark_session, spark_context, fileSize = build_spark_session(hdfs_path, file_path, verbose=verbose)\n",
    "    df = load_data(spark_session, hdfs_path, file_path, fileSize, verbose=verbose)\n",
    "    train_data, test_data, no_samples = filter_and_split_data(df, verbose=verbose)\n",
    "    pre_time = time.time()\n",
    "    pre_time = pre_time - start_time\n",
    "    print(f\"Pre-processing time: {pre_time:.4f}\")\n",
    "\n",
    "    pre_pipe = pre_processing_pipe()\n",
    "    model_rf = random_forest(train_data, pre_pipe)\n",
    "    rf_time  = time.time()\n",
    "    rf_time  = rf_time - start_time - pre_time\n",
    "    accuracy_rf = model_eval(model_rf, test_data, description=\"Random forest classifier\")\n",
    "    print(f\"Random forest time: {rf_time:.4f}\")\n",
    "\n",
    "    \"\"\" pre_pipe = pre_processing_pipe()\n",
    "    model_lr = logistic_regression(train_data, pre_pipe)\n",
    "    lr_time  = time.time()\n",
    "    lr_time  = lr_time - start_time - rf_time\n",
    "    accuracy_lr = model_eval(model_lr, test_data, description=\"Logistic regression classifier\")\n",
    "    print(f\"Logistic regression time: {lr_time:.4f}\") \"\"\"\n",
    "    accuracy_lr = 0\n",
    "\n",
    "    executor_cores = spark_session.conf.get(\"spark.executor.cores\")\n",
    "    executor_memory = spark_session.conf.get(\"spark.executor.memory\")\n",
    "    spark_context.stop()\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    reddit_50_samples[i] = no_samples\n",
    "    reddit_50_time[i]    = execution_time\n",
    "    reddit_50_rf_acc[i]  = accuracy_rf\n",
    "    reddit_50_lr_acc[i]  = accuracy_lr\n",
    "\n",
    "    print_results(file_path=file_path, fileSize=bytes_to_gb(fileSize), no_samples=no_samples, executor_cores=executor_cores, \n",
    "                executor_memory=executor_memory, execution_time=execution_time, \n",
    "                accuracy_rf=accuracy_rf, accuracy_lr=accuracy_lr)\n",
    "            \n",
    "print(reddit_100_samples.mean())\n",
    "print(reddit_100_samples)\n",
    "\n",
    "print(reddit_100_time.mean())\n",
    "print(reddit_100_time)\n",
    "\n",
    "print(reddit_100_rf_acc.mean())\n",
    "print(reddit_100_rf_acc)\n",
    "\n",
    "print(reddit_100_lr_acc.mean())\n",
    "print(reddit_100_lr_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd1fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=False\n",
    "file_path = \"/data/reddit/reddit_200k.json\"\n",
    "reddit_200_samples = np.zeros(5)\n",
    "reddit_200_time    = np.zeros(5)\n",
    "reddit_200_rf_acc  = np.zeros(5)\n",
    "reddit_200_lr_acc  = np.zeros(5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Run: {i}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    spark_session, spark_context, fileSize = build_spark_session(hdfs_path, file_path, verbose=verbose)\n",
    "    df = load_data(spark_session, hdfs_path, file_path, fileSize, verbose=verbose)\n",
    "    train_data, test_data, no_samples = filter_and_split_data(df, verbose=verbose)\n",
    "    pre_time = time.time()\n",
    "    pre_time = pre_time - start_time\n",
    "    print(f\"Pre-processing time: {pre_time:.4f}\")\n",
    "\n",
    "    pre_pipe = pre_processing_pipe()\n",
    "    model_rf = random_forest(train_data, pre_pipe)\n",
    "    rf_time  = time.time()\n",
    "    rf_time  = rf_time - start_time - pre_time\n",
    "    accuracy_rf = model_eval(model_rf, test_data, description=\"Random forest classifier\")\n",
    "    print(f\"Random forest time: {rf_time:.4f}\")\n",
    "\n",
    "    \"\"\" pre_pipe = pre_processing_pipe()\n",
    "    model_lr = logistic_regression(train_data, pre_pipe)\n",
    "    lr_time  = time.time()\n",
    "    lr_time  = lr_time - start_time - rf_time\n",
    "    accuracy_lr = model_eval(model_lr, test_data, description=\"Logistic regression classifier\")\n",
    "    print(f\"Logistic regression time: {lr_time:.4f}\") \"\"\"\n",
    "    accuracy_lr = 0\n",
    "\n",
    "    executor_cores = spark_session.conf.get(\"spark.executor.cores\")\n",
    "    executor_memory = spark_session.conf.get(\"spark.executor.memory\")\n",
    "    spark_context.stop()\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    reddit_50_samples[i] = no_samples\n",
    "    reddit_50_time[i]    = execution_time\n",
    "    reddit_50_rf_acc[i]  = accuracy_rf\n",
    "    reddit_50_lr_acc[i]  = accuracy_lr\n",
    "\n",
    "    print_results(file_path=file_path, fileSize=bytes_to_gb(fileSize), no_samples=no_samples, executor_cores=executor_cores, \n",
    "                executor_memory=executor_memory, execution_time=execution_time, \n",
    "                accuracy_rf=accuracy_rf, accuracy_lr=accuracy_lr)\n",
    "            \n",
    "print(reddit_200_samples.mean())\n",
    "print(reddit_200_samples)\n",
    "\n",
    "print(reddit_200_time.mean())\n",
    "print(reddit_200_time)\n",
    "\n",
    "print(reddit_200_rf_acc.mean())\n",
    "print(reddit_200_rf_acc)\n",
    "\n",
    "print(reddit_200_lr_acc.mean())\n",
    "print(reddit_200_lr_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2fda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=False\n",
    "file_path = \"/data/reddit/reddit_500k.json\"\n",
    "reddit_500_samples = np.zeros(5)\n",
    "reddit_500_time    = np.zeros(5)\n",
    "reddit_500_rf_acc  = np.zeros(5)\n",
    "reddit_500_lr_acc  = np.zeros(5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Run: {i}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    spark_session, spark_context, fileSize = build_spark_session(hdfs_path, file_path, verbose=verbose)\n",
    "    df = load_data(spark_session, hdfs_path, file_path, fileSize, verbose=verbose)\n",
    "    train_data, test_data, no_samples = filter_and_split_data(df, verbose=verbose)\n",
    "    pre_time = time.time()\n",
    "    pre_time = pre_time - start_time\n",
    "    print(f\"Pre-processing time: {pre_time:.4f}\")\n",
    "\n",
    "    pre_pipe = pre_processing_pipe()\n",
    "    model_rf = random_forest(train_data, pre_pipe)\n",
    "    rf_time  = time.time()\n",
    "    rf_time  = rf_time - start_time - pre_time\n",
    "    accuracy_rf = model_eval(model_rf, test_data, description=\"Random forest classifier\")\n",
    "    print(f\"Random forest time: {rf_time:.4f}\")\n",
    "\n",
    "    \"\"\" pre_pipe = pre_processing_pipe()\n",
    "    model_lr = logistic_regression(train_data, pre_pipe)\n",
    "    lr_time  = time.time()\n",
    "    lr_time  = lr_time - start_time - rf_time\n",
    "    accuracy_lr = model_eval(model_lr, test_data, description=\"Logistic regression classifier\")\n",
    "    print(f\"Logistic regression time: {lr_time:.4f}\") \"\"\"\n",
    "    accuracy_lr = 0\n",
    "\n",
    "    executor_cores = spark_session.conf.get(\"spark.executor.cores\")\n",
    "    executor_memory = spark_session.conf.get(\"spark.executor.memory\")\n",
    "    spark_context.stop()\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    reddit_50_samples[i] = no_samples\n",
    "    reddit_50_time[i]    = execution_time\n",
    "    reddit_50_rf_acc[i]  = accuracy_rf\n",
    "    reddit_50_lr_acc[i]  = accuracy_lr\n",
    "\n",
    "    print_results(file_path=file_path, fileSize=bytes_to_gb(fileSize), no_samples=no_samples, executor_cores=executor_cores, \n",
    "                executor_memory=executor_memory, execution_time=execution_time, \n",
    "                accuracy_rf=accuracy_rf, accuracy_lr=accuracy_lr)\n",
    "            \n",
    "print(reddit_500_samples.mean())\n",
    "print(reddit_500_samples)\n",
    "\n",
    "print(reddit_500_time.mean())\n",
    "print(reddit_500_time)\n",
    "\n",
    "print(reddit_500_rf_acc.mean())\n",
    "print(reddit_500_rf_acc)\n",
    "\n",
    "print(reddit_500_lr_acc.mean())\n",
    "print(reddit_500_lr_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=False\n",
    "file_path = \"/data/reddit/corpus-webis-tldr-17.json\"\n",
    "reddit_full_samples = np.zeros(5)\n",
    "reddit_full_time    = np.zeros(5)\n",
    "reddit_full_rf_acc  = np.zeros(5)\n",
    "reddit_full_lr_acc  = np.zeros(5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Run: {i}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    spark_session, spark_context, fileSize = build_spark_session(hdfs_path, file_path, verbose=verbose)\n",
    "    df = load_data(spark_session, hdfs_path, file_path, fileSize, verbose=verbose)\n",
    "    train_data, test_data, no_samples = filter_and_split_data(df, verbose=verbose)\n",
    "    pre_time = time.time()\n",
    "    pre_time = pre_time - start_time\n",
    "    print(f\"Pre-processing time: {pre_time:.4f}\")\n",
    "\n",
    "    pre_pipe = pre_processing_pipe()\n",
    "    model_rf = random_forest(train_data, pre_pipe)\n",
    "    rf_time  = time.time()\n",
    "    rf_time  = rf_time - start_time - pre_time\n",
    "    accuracy_rf = model_eval(model_rf, test_data, description=\"Random forest classifier\")\n",
    "    print(f\"Random forest time: {rf_time:.4f}\")\n",
    "\n",
    "    \"\"\" pre_pipe = pre_processing_pipe()\n",
    "    model_lr = logistic_regression(train_data, pre_pipe)\n",
    "    lr_time  = time.time()\n",
    "    lr_time  = lr_time - start_time - rf_time\n",
    "    accuracy_lr = model_eval(model_lr, test_data, description=\"Logistic regression classifier\")\n",
    "    print(f\"Logistic regression time: {lr_time:.4f}\") \"\"\"\n",
    "    accuracy_lr = 0\n",
    "\n",
    "    executor_cores = spark_session.conf.get(\"spark.executor.cores\")\n",
    "    executor_memory = spark_session.conf.get(\"spark.executor.memory\")\n",
    "    spark_context.stop()\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    reddit_50_samples[i] = no_samples\n",
    "    reddit_50_time[i]    = execution_time\n",
    "    reddit_50_rf_acc[i]  = accuracy_rf\n",
    "    reddit_50_lr_acc[i]  = accuracy_lr\n",
    "\n",
    "    print_results(file_path=file_path, fileSize=bytes_to_gb(fileSize), no_samples=no_samples, executor_cores=executor_cores, \n",
    "                executor_memory=executor_memory, execution_time=execution_time, \n",
    "                accuracy_rf=accuracy_rf, accuracy_lr=accuracy_lr)\n",
    "            \n",
    "print(reddit_full_samples.mean())\n",
    "print(reddit_full_samples)\n",
    "\n",
    "print(reddit_full_time.mean())\n",
    "print(reddit_full_time)\n",
    "\n",
    "print(reddit_full_rf_acc.mean())\n",
    "print(reddit_full_rf_acc)\n",
    "\n",
    "print(reddit_full_lr_acc.mean())\n",
    "print(reddit_full_lr_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092dd2b-e265-41fe-a832-38c8d38d8f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/11 07:54:21 ERROR Instrumentation: org.apache.spark.SparkException: Job 109 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c12129-08fd-48d2-bdac-983d5af5c9b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652336a3-1a25-4616-81ce-ac2cd1e1874b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7ba16-375f-4236-bbee-1776bc00f0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
